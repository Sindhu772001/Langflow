{"name": "Ticket_Classification_with_RAG", "description": "Building Intelligent Interactions.", "icon": null, "icon_bg_color": null, "gradient": null, "data": {"nodes": [{"id": "ChatInput-meIRY", "type": "genericNode", "position": {"x": -375.2021505741286, "y": 36.147773421959286}, "data": {"node": {"template": {"_type": "Component", "files": {"trace_as_metadata": true, "file_path": "", "fileTypes": ["txt", "md", "mdx", "csv", "json", "yaml", "yml", "xml", "html", "htm", "pdf", "docx", "py", "sh", "sql", "js", "ts", "tsx", "jpg", "jpeg", "png", "bmp", "image"], "list": true, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "files", "value": "", "display_name": "Files", "advanced": true, "dynamic": false, "info": "Files to be sent with the message.", "title_case": false, "type": "file", "_input_type": "FileInput"}, "background_color": {"tool_mode": false, "trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "background_color", "value": "", "display_name": "Background Color", "advanced": true, "input_types": ["Message"], "dynamic": false, "info": "The background color of the icon.", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "chat_icon": {"tool_mode": false, "trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "chat_icon", "value": "", "display_name": "Icon", "advanced": true, "input_types": ["Message"], "dynamic": false, "info": "The icon of the message.", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "code": {"type": "code", "required": true, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "from langflow.base.data.utils import IMG_FILE_TYPES, TEXT_FILE_TYPES\nfrom langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.io import (\n    DropdownInput,\n    FileInput,\n    MessageTextInput,\n    MultilineInput,\n    Output,\n)\nfrom langflow.schema.message import Message\nfrom langflow.utils.constants import (\n    MESSAGE_SENDER_AI,\n    MESSAGE_SENDER_NAME_USER,\n    MESSAGE_SENDER_USER,\n)\n\n\nclass ChatInput(ChatComponent):\n    display_name = \"Chat Input\"\n    description = \"Get chat inputs from the Playground.\"\n    icon = \"MessagesSquare\"\n    name = \"ChatInput\"\n    minimized = True\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            value=\"\",\n            info=\"Message to be passed as input.\",\n            input_types=[],\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_USER,\n            info=\"Type of sender.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_USER,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        FileInput(\n            name=\"files\",\n            display_name=\"Files\",\n            file_types=TEXT_FILE_TYPES + IMG_FILE_TYPES,\n            info=\"Files to be sent with the message.\",\n            advanced=True,\n            is_list=True,\n        ),\n        MessageTextInput(\n            name=\"background_color\",\n            display_name=\"Background Color\",\n            info=\"The background color of the icon.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"chat_icon\",\n            display_name=\"Icon\",\n            info=\"The icon of the message.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"text_color\",\n            display_name=\"Text Color\",\n            info=\"The text color of the name\",\n            advanced=True,\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    async def message_response(self) -> Message:\n        background_color = self.background_color\n        text_color = self.text_color\n        icon = self.chat_icon\n\n        message = await Message.create(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n            files=self.files,\n            properties={\n                \"background_color\": background_color,\n                \"text_color\": text_color,\n                \"icon\": icon,\n            },\n        )\n        if self.session_id and isinstance(message, Message) and self.should_store_message:\n            stored_message = await self.send_message(\n                message,\n            )\n            self.message.value = stored_message\n            message = stored_message\n\n        self.status = message\n        return message\n", "fileTypes": [], "file_path": "", "password": false, "name": "code", "advanced": true, "dynamic": true, "info": "", "load_from_db": false, "title_case": false}, "input_value": {"tool_mode": false, "trace_as_input": true, "multiline": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "input_value", "value": "Subject: Access Granted \u2013 Project Docs Shared Folder  Dear John Doe,  Your request for access to the Project Docs shared folder has been processed. You now have Read & Edit permissions.  If you experience any issues accessing the folder, please try logging out and back in. If the issue persists, feel free to reach out.  Best regards, Michael Carter", "display_name": "Text", "advanced": false, "input_types": [], "dynamic": false, "info": "Message to be passed as input.", "title_case": false, "type": "str", "_input_type": "MultilineInput"}, "sender": {"tool_mode": false, "trace_as_metadata": true, "options": ["Machine", "User"], "options_metadata": [], "combobox": false, "dialog_inputs": {}, "required": false, "placeholder": "", "show": true, "name": "sender", "value": "User", "display_name": "Sender Type", "advanced": true, "dynamic": false, "info": "Type of sender.", "title_case": false, "type": "str", "_input_type": "DropdownInput"}, "sender_name": {"tool_mode": false, "trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "sender_name", "value": "User", "display_name": "Sender Name", "advanced": true, "input_types": ["Message"], "dynamic": false, "info": "Name of the sender.", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "session_id": {"tool_mode": false, "trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "session_id", "value": "", "display_name": "Session ID", "advanced": true, "input_types": ["Message"], "dynamic": false, "info": "The session ID of the chat. If empty, the current session ID parameter will be used.", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "should_store_message": {"tool_mode": false, "trace_as_metadata": true, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "should_store_message", "value": true, "display_name": "Store Messages", "advanced": true, "dynamic": false, "info": "Store the message in the history.", "title_case": false, "type": "bool", "_input_type": "BoolInput"}, "text_color": {"tool_mode": false, "trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "text_color", "value": "", "display_name": "Text Color", "advanced": true, "input_types": ["Message"], "dynamic": false, "info": "The text color of the name", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}}, "description": "Get chat inputs from the Playground.", "icon": "MessagesSquare", "base_classes": ["Message"], "display_name": "Chat Input", "documentation": "", "minimized": true, "custom_fields": {}, "output_types": [], "pinned": false, "conditional_paths": [], "frozen": false, "outputs": [{"types": ["Message"], "selected": "Message", "name": "message", "display_name": "Message", "method": "message_response", "value": "__UNDEFINED__", "cache": true, "allows_loop": false}], "field_order": ["input_value", "should_store_message", "sender", "sender_name", "session_id", "files", "background_color", "chat_icon", "text_color"], "beta": false, "legacy": false, "edited": false, "metadata": {}, "tool_mode": false, "lf_version": "1.1.3"}, "showNode": true, "type": "ChatInput", "id": "ChatInput-meIRY"}, "selected": false, "measured": {"width": 320, "height": 229}, "dragging": false}, {"id": "OpenAIModel-CriS5", "type": "genericNode", "position": {"x": 503.53501733525366, "y": -353.0657754714991}, "data": {"node": {"template": {"_type": "Component", "api_key": {"load_from_db": false, "required": true, "placeholder": "", "show": true, "name": "api_key", "value": null, "display_name": "OpenAI API Key", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "The OpenAI API Key to use for the OpenAI model.", "title_case": false, "password": true, "type": "str", "_input_type": "SecretStrInput"}, "code": {"type": "code", "required": true, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "from langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.base.models.openai_constants import OPENAI_MODEL_NAMES\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import BoolInput, DictInput, DropdownInput, IntInput, SecretStrInput, SliderInput, StrInput\n\n\nclass OpenAIModelComponent(LCModelComponent):\n    display_name = \"OpenAI\"\n    description = \"Generates text using OpenAI LLMs.\"\n    icon = \"OpenAI\"\n    name = \"OpenAIModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n            range_spec=RangeSpec(min=0, max=128000),\n        ),\n        DictInput(\n            name=\"model_kwargs\",\n            display_name=\"Model Kwargs\",\n            advanced=True,\n            info=\"Additional keyword arguments to pass to the model.\",\n        ),\n        BoolInput(\n            name=\"json_mode\",\n            display_name=\"JSON Mode\",\n            advanced=True,\n            info=\"If True, it will output JSON regardless of passing a schema.\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            options=OPENAI_MODEL_NAMES,\n            value=OPENAI_MODEL_NAMES[0],\n        ),\n        StrInput(\n            name=\"openai_api_base\",\n            display_name=\"OpenAI API Base\",\n            advanced=True,\n            info=\"The base URL of the OpenAI API. \"\n            \"Defaults to https://api.openai.com/v1. \"\n            \"You can change this to use other APIs like JinaChat, LocalAI and Prem.\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"OpenAI API Key\",\n            info=\"The OpenAI API Key to use for the OpenAI model.\",\n            advanced=False,\n            value=\"OPENAI_API_KEY\",\n            required=True,\n        ),\n        SliderInput(\n            name=\"temperature\", display_name=\"Temperature\", value=0.1, range_spec=RangeSpec(min=0, max=1, step=0.01)\n        ),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n        IntInput(\n            name=\"max_retries\",\n            display_name=\"Max Retries\",\n            info=\"The maximum number of retries to make when generating.\",\n            advanced=True,\n            value=5,\n        ),\n        IntInput(\n            name=\"timeout\",\n            display_name=\"Timeout\",\n            info=\"The timeout for requests to OpenAI completion API.\",\n            advanced=True,\n            value=700,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        openai_api_key = self.api_key\n        temperature = self.temperature\n        model_name: str = self.model_name\n        max_tokens = self.max_tokens\n        model_kwargs = self.model_kwargs or {}\n        openai_api_base = self.openai_api_base or \"https://api.openai.com/v1\"\n        json_mode = self.json_mode\n        seed = self.seed\n        max_retries = self.max_retries\n        timeout = self.timeout\n\n        api_key = SecretStr(openai_api_key).get_secret_value() if openai_api_key else None\n        output = ChatOpenAI(\n            max_tokens=max_tokens or None,\n            model_kwargs=model_kwargs,\n            model=model_name,\n            base_url=openai_api_base,\n            api_key=api_key,\n            temperature=temperature if temperature is not None else 0.1,\n            seed=seed,\n            max_retries=max_retries,\n            request_timeout=timeout,\n        )\n        if json_mode:\n            output = output.bind(response_format={\"type\": \"json_object\"})\n\n        return output\n\n    def _get_exception_message(self, e: Exception):\n        \"\"\"Get a message from an OpenAI exception.\n\n        Args:\n            e (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n        try:\n            from openai import BadRequestError\n        except ImportError:\n            return None\n        if isinstance(e, BadRequestError):\n            message = e.body.get(\"message\")\n            if message:\n                return message\n        return None\n", "fileTypes": [], "file_path": "", "password": false, "name": "code", "advanced": true, "dynamic": true, "info": "", "load_from_db": false, "title_case": false}, "input_value": {"trace_as_input": true, "tool_mode": false, "trace_as_metadata": true, "load_from_db": false, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "input_value", "value": "", "display_name": "Input", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "", "title_case": false, "type": "str", "_input_type": "MessageInput"}, "json_mode": {"tool_mode": false, "trace_as_metadata": true, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "json_mode", "value": false, "display_name": "JSON Mode", "advanced": true, "dynamic": false, "info": "If True, it will output JSON regardless of passing a schema.", "title_case": false, "type": "bool", "_input_type": "BoolInput"}, "max_retries": {"tool_mode": false, "trace_as_metadata": true, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "max_retries", "value": 5, "display_name": "Max Retries", "advanced": true, "dynamic": false, "info": "The maximum number of retries to make when generating.", "title_case": false, "type": "int", "_input_type": "IntInput"}, "max_tokens": {"tool_mode": false, "trace_as_metadata": true, "range_spec": {"step_type": "float", "min": 0, "max": 128000, "step": 0.1}, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "max_tokens", "value": "", "display_name": "Max Tokens", "advanced": true, "dynamic": false, "info": "The maximum number of tokens to generate. Set to 0 for unlimited tokens.", "title_case": false, "type": "int", "_input_type": "IntInput"}, "model_kwargs": {"tool_mode": false, "trace_as_input": true, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "model_kwargs", "value": {}, "display_name": "Model Kwargs", "advanced": true, "dynamic": false, "info": "Additional keyword arguments to pass to the model.", "title_case": false, "type": "dict", "_input_type": "DictInput"}, "model_name": {"tool_mode": false, "trace_as_metadata": true, "options": ["gpt-4o-mini", "gpt-4o", "gpt-4-turbo", "gpt-4-turbo-preview", "gpt-4", "gpt-3.5-turbo", "gpt-3.5-turbo-0125", "unsloth-llama3.1"], "options_metadata": [], "combobox": false, "dialog_inputs": {}, "required": false, "placeholder": "", "show": true, "name": "model_name", "value": "unsloth-llama3.1", "display_name": "Model Name", "advanced": false, "dynamic": false, "info": "", "title_case": false, "type": "str", "_input_type": "DropdownInput", "load_from_db": false}, "openai_api_base": {"tool_mode": false, "trace_as_metadata": true, "load_from_db": false, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "openai_api_base", "value": "https://dev-vllm-unsloth.bautomate.com/v1", "display_name": "OpenAI API Base", "advanced": true, "dynamic": false, "info": "The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.", "title_case": false, "type": "str", "_input_type": "StrInput"}, "seed": {"tool_mode": false, "trace_as_metadata": true, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "seed", "value": 1, "display_name": "Seed", "advanced": true, "dynamic": false, "info": "The seed controls the reproducibility of the job.", "title_case": false, "type": "int", "_input_type": "IntInput"}, "stream": {"tool_mode": false, "trace_as_metadata": true, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "stream", "value": false, "display_name": "Stream", "advanced": false, "dynamic": false, "info": "Stream the response from the model. Streaming works only in Chat.", "title_case": false, "type": "bool", "_input_type": "BoolInput"}, "system_message": {"tool_mode": false, "trace_as_input": true, "multiline": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "system_message", "value": "admin", "display_name": "System Message", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "System message to pass to the model.", "title_case": false, "type": "str", "_input_type": "MultilineInput"}, "temperature": {"tool_mode": false, "min_label": "", "max_label": "", "min_label_icon": "", "max_label_icon": "", "slider_buttons": false, "slider_buttons_options": [], "slider_input": false, "range_spec": {"step_type": "float", "min": 0, "max": 1, "step": 0.01}, "required": false, "placeholder": "", "show": true, "name": "temperature", "value": 0.1, "display_name": "Temperature", "advanced": false, "dynamic": false, "info": "", "title_case": false, "type": "slider", "_input_type": "SliderInput"}, "timeout": {"tool_mode": false, "trace_as_metadata": true, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "timeout", "value": 700, "display_name": "Timeout", "advanced": true, "dynamic": false, "info": "The timeout for requests to OpenAI completion API.", "title_case": false, "type": "int", "_input_type": "IntInput"}}, "description": "Generates text using OpenAI LLMs.", "icon": "OpenAI", "base_classes": ["LanguageModel", "Message"], "display_name": "OpenAI", "documentation": "", "minimized": false, "custom_fields": {}, "output_types": [], "pinned": false, "conditional_paths": [], "frozen": false, "outputs": [{"types": ["Message"], "selected": "Message", "name": "text_output", "hidden": null, "display_name": "Message", "method": "text_response", "value": "__UNDEFINED__", "cache": true, "required_inputs": [], "allows_loop": false, "tool_mode": true}, {"types": ["LanguageModel"], "selected": "LanguageModel", "name": "model_output", "hidden": null, "display_name": "Language Model", "method": "build_model", "value": "__UNDEFINED__", "cache": true, "required_inputs": ["api_key"], "allows_loop": false, "tool_mode": true}], "field_order": ["input_value", "system_message", "stream", "max_tokens", "model_kwargs", "json_mode", "model_name", "openai_api_base", "api_key", "temperature", "seed", "max_retries", "timeout"], "beta": false, "legacy": false, "edited": false, "metadata": {}, "tool_mode": false}, "showNode": true, "type": "OpenAIModel", "id": "OpenAIModel-CriS5"}, "selected": false, "measured": {"width": 320, "height": 653}, "dragging": false}, {"id": "Prompt-MQYzX", "type": "genericNode", "position": {"x": 29.28538425703769, "y": -198.25046473720718}, "data": {"node": {"template": {"_type": "Component", "code": {"type": "code", "required": true, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import MessageTextInput, Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n        MessageTextInput(\n            name=\"tool_placeholder\",\n            display_name=\"Tool Placeholder\",\n            tool_mode=True,\n            advanced=True,\n            info=\"A placeholder input for tool mode.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    async def update_frontend_node(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = await super().update_frontend_node(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n", "fileTypes": [], "file_path": "", "password": false, "name": "code", "advanced": true, "dynamic": true, "info": "", "load_from_db": false, "title_case": false}, "template": {"tool_mode": false, "trace_as_input": true, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "template", "value": "You are an intelligent ITSM ticket classifier and extractor. \nYour task is to analyze the provided ITSM ticket, which may be an email or an automatically generated monitoring log. \nIf the ticket is an automatically generated monitoring log, treat it as an email for classification purposes. \nYour responsibilities include determining its severity, dynamically identifying the issue category, and suggesting an appropriate action plan based on its content. Even if specific details are not explicitly mentioned, you should infer and classify the ticket, determine its category, and recommend suitable actions.\n\nInstructions:\n\n1. Classification: Classify the severity of the issue into one of the following categories:\n   - L1 (Low): Informational or non-urgent tasks. Examples: Minor application error with no business impact (e.g., UI misalignment in internal software), scheduling firmware updates for non-critical devices, requests for access to shared folders, general inquiries regarding system documentation or training materials.\n   - L2 (Medium): Minor issues or low priority. Examples: Network latency impacting specific users (e.g., slow intranet performance), analyzing recurring performance degradation in an internal application, printer configuration issues causing minor delays, non-critical software bug fixes, request for software installation for new hires.\n   - L3 (High): Issues that require technical intervention, but are not critical. Examples: VPN connectivity issues affecting a remote team, implementing a network topology change that affects active users, email delivery failure for multiple users, security patch updates for vulnerability mitigation, configuration errors in firewall rules impacting access control.\n   - L4 (Critical): Major issues that need immediate resolution. Examples: Data center outage causing service unavailability for all users, emergency patch deployment to fix a vulnerability exploited in an ongoing attack, server hardware failure causing downtime for critical systems, database corruption resulting in data loss and service disruption, ransomware attack impacting business operations.\n\n2. Categorize the following components appropriately:\n\n- Switch/Router(Physical Components): Should we categorize by physical component? E.g. Fans, PSU etc.\n- UPS(Physical Components): Should we categorize by physical component? E.g. Battery, Network Card etc.\n- Physical Windows servers(Physical components): Should we categorize by physical component? E.g. Fans, PSU etc.\n- Windows Server: Should we categorize by services? E.g. CPU, Memory, Disk space etc.\n- ESXi servers(Physical Component): Should we categorize by physical component? E.g. Fans, PSU etc.\n- Storage appliances: Should we categorize by physical component? E.g. Fans, PSU etc.\n- Backup Jobs: Should we categorize by vendor? Veeam, Barracuda etc.\n- Wireless Access Points: Should we categorize by vendor? Meraki, Aruba etc.\n- Security Management: Should we categorize by vendor? Crowdstrike, Cisco etc.\n\n\n3. Priority: Based on the classification and severity, determine the priority as:\n   - High\n   - Medium\n   - Low\n\n4. Suggested Action: Based on the category and severity of the issue, suggest an appropriate action plan.\n\n5. Action Plan: Provide the steps or actions needed to resolve the issue.\n\n6. Information Field: Dynamically retrieve all the relevant details from the email. The information field must be flexible and adapt to the structure of each incoming email. These fields will be populated dynamically depending on the content of the email and they can change with each ticket.\n\nOutput Structure:\nThe output should follow this structure:\n\n[\n    {{\"classification\": \"L1 (Low) / L2 (Medium) / L3 (High) / L4 (Critical)\"}},\n    {{\"subject\": \"Ticket issue summary\"}},\n    {{\"description\": \"Detailed issue description\"}},\n    {{\"category\": \"Switch or Router (Physical Components) / UPS (Physical Components) / Physical Windows Servers (Physical Components) / Windows Server / ESXi Servers (Physical Components) / Storage Appliances (Physical Components) / Backup Jobs / Wireless Access Points / Security Management\"}},\n    {{\"priority\": \"High / Medium / Low\"}},\n    {{\"suggestedAction\": \"Suggested actions to resolve the issue\"}},\n    {{\"information\": \"field1\": \"value1\"/\"field2\": \"value2\"}},\n    {{\"actionPlan\": \"step1\": \"Step description\"/\"step2\": \"Step description\"}}\n    ]}}\n]\n\nNotes:\n- The information field must be dynamically generated based on the contents of the email and can vary with each ticket. The fields and values in the information field will differ based on the specific email received.\n- The system should automatically classify, categorize and provide suggested actions without explicit instructions in the email.\n- The structure and contents of the information field should adapt to the ticket details in each email.\n- If any information is missing in the input email, return as empty string or NA.\n- Return only the above json format and not any other explanation\n{content}", "display_name": "Template", "advanced": false, "dynamic": false, "info": "", "title_case": false, "type": "prompt", "_input_type": "PromptInput"}, "tool_placeholder": {"tool_mode": true, "trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "tool_placeholder", "value": "", "display_name": "Tool Placeholder", "advanced": true, "input_types": ["Message"], "dynamic": false, "info": "A placeholder input for tool mode.", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "content": {"field_type": "str", "required": false, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "", "fileTypes": [], "file_path": "", "name": "content", "display_name": "content", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "", "load_from_db": false, "title_case": false, "type": "str"}}, "description": "Create a prompt template with dynamic variables.", "icon": "prompts", "is_input": null, "is_output": null, "is_composition": null, "base_classes": ["Message"], "name": "", "display_name": "Prompt", "documentation": "", "minimized": false, "custom_fields": {"template": ["content"]}, "output_types": [], "full_path": null, "pinned": false, "conditional_paths": [], "frozen": false, "outputs": [{"types": ["Message"], "selected": "Message", "name": "prompt", "hidden": null, "display_name": "Prompt Message", "method": "build_prompt", "value": "__UNDEFINED__", "cache": true, "required_inputs": null, "allows_loop": false}], "field_order": ["template", "tool_placeholder"], "beta": false, "legacy": false, "error": null, "edited": false, "metadata": {}, "tool_mode": false, "lf_version": "1.1.3"}, "showNode": true, "type": "Prompt", "id": "Prompt-MQYzX"}, "selected": false, "measured": {"width": 320, "height": 337}, "dragging": false}, {"id": "JSONCleaner-1mD4u", "type": "genericNode", "position": {"x": 882.170337288613, "y": -46.14359413284979}, "data": {"node": {"template": {"_type": "Component", "code": {"type": "code", "required": true, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "import json\nimport unicodedata\n\nfrom langflow.custom import Component\nfrom langflow.inputs import BoolInput, MessageTextInput\nfrom langflow.schema.message import Message\nfrom langflow.template import Output\n\n\nclass JSONCleaner(Component):\n    icon = \"braces\"\n    display_name = \"JSON Cleaner\"\n    description = (\n        \"Cleans the messy and sometimes incorrect JSON strings produced by LLMs \"\n        \"so that they are fully compliant with the JSON spec.\"\n    )\n\n    inputs = [\n        MessageTextInput(\n            name=\"json_str\", display_name=\"JSON String\", info=\"The JSON string to be cleaned.\", required=True\n        ),\n        BoolInput(\n            name=\"remove_control_chars\",\n            display_name=\"Remove Control Characters\",\n            info=\"Remove control characters from the JSON string.\",\n            required=False,\n        ),\n        BoolInput(\n            name=\"normalize_unicode\",\n            display_name=\"Normalize Unicode\",\n            info=\"Normalize Unicode characters in the JSON string.\",\n            required=False,\n        ),\n        BoolInput(\n            name=\"validate_json\",\n            display_name=\"Validate JSON\",\n            info=\"Validate the JSON string to ensure it is well-formed.\",\n            required=False,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Cleaned JSON String\", name=\"output\", method=\"clean_json\"),\n    ]\n\n    def clean_json(self) -> Message:\n        try:\n            from json_repair import repair_json\n        except ImportError as e:\n            msg = \"Could not import the json_repair package. Please install it with `pip install json_repair`.\"\n            raise ImportError(msg) from e\n\n        \"\"\"Clean the input JSON string based on provided options and return the cleaned JSON string.\"\"\"\n        json_str = self.json_str\n        remove_control_chars = self.remove_control_chars\n        normalize_unicode = self.normalize_unicode\n        validate_json = self.validate_json\n\n        start = json_str.find(\"{\")\n        end = json_str.rfind(\"}\")\n        if start == -1 or end == -1:\n            msg = \"Invalid JSON string: Missing '{' or '}'\"\n            raise ValueError(msg)\n        try:\n            json_str = json_str[start : end + 1]\n\n            if remove_control_chars:\n                json_str = self._remove_control_characters(json_str)\n            if normalize_unicode:\n                json_str = self._normalize_unicode(json_str)\n            if validate_json:\n                json_str = self._validate_json(json_str)\n\n            cleaned_json_str = repair_json(json_str)\n            result = str(cleaned_json_str)\n\n            self.status = result\n            return Message(text=result)\n        except Exception as e:\n            msg = f\"Error cleaning JSON string: {e}\"\n            raise ValueError(msg) from e\n\n    def _remove_control_characters(self, s: str) -> str:\n        \"\"\"Remove control characters from the string.\"\"\"\n        return s.translate(self.translation_table)\n\n    def _normalize_unicode(self, s: str) -> str:\n        \"\"\"Normalize Unicode characters in the string.\"\"\"\n        return unicodedata.normalize(\"NFC\", s)\n\n    def _validate_json(self, s: str) -> str:\n        \"\"\"Validate the JSON string.\"\"\"\n        try:\n            json.loads(s)\n        except json.JSONDecodeError as e:\n            msg = f\"Invalid JSON string: {e}\"\n            raise ValueError(msg) from e\n        return s\n\n    def __init__(self, *args, **kwargs):\n        # Create a translation table that maps control characters to None\n        super().__init__(*args, **kwargs)\n        self.translation_table = str.maketrans(\"\", \"\", \"\".join(chr(i) for i in range(32)) + chr(127))\n", "fileTypes": [], "file_path": "", "password": false, "name": "code", "advanced": true, "dynamic": true, "info": "", "load_from_db": false, "title_case": false}, "json_str": {"tool_mode": false, "trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "list_add_label": "Add More", "required": true, "placeholder": "", "show": true, "name": "json_str", "value": "", "display_name": "JSON String", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "The JSON string to be cleaned.", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "normalize_unicode": {"tool_mode": false, "trace_as_metadata": true, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "normalize_unicode", "value": false, "display_name": "Normalize Unicode", "advanced": false, "dynamic": false, "info": "Normalize Unicode characters in the JSON string.", "title_case": false, "type": "bool", "_input_type": "BoolInput"}, "remove_control_chars": {"tool_mode": false, "trace_as_metadata": true, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "remove_control_chars", "value": false, "display_name": "Remove Control Characters", "advanced": false, "dynamic": false, "info": "Remove control characters from the JSON string.", "title_case": false, "type": "bool", "_input_type": "BoolInput"}, "validate_json": {"tool_mode": false, "trace_as_metadata": true, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "validate_json", "value": false, "display_name": "Validate JSON", "advanced": false, "dynamic": false, "info": "Validate the JSON string to ensure it is well-formed.", "title_case": false, "type": "bool", "_input_type": "BoolInput"}}, "description": "Cleans the messy and sometimes incorrect JSON strings produced by LLMs so that they are fully compliant with the JSON spec.", "icon": "braces", "base_classes": ["Message"], "display_name": "JSON Cleaner", "documentation": "", "minimized": false, "custom_fields": {}, "output_types": [], "pinned": false, "conditional_paths": [], "frozen": false, "outputs": [{"types": ["Message"], "selected": "Message", "name": "output", "display_name": "Cleaned JSON String", "method": "clean_json", "value": "__UNDEFINED__", "cache": true, "allows_loop": false}], "field_order": ["json_str", "remove_control_chars", "normalize_unicode", "validate_json"], "beta": false, "legacy": false, "edited": false, "metadata": {}, "tool_mode": false, "category": "processing", "key": "JSONCleaner", "score": 0.007568328950209746, "lf_version": "1.1.3"}, "showNode": true, "type": "JSONCleaner", "id": "JSONCleaner-1mD4u"}, "selected": false, "measured": {"width": 320, "height": 395}}, {"id": "ChatOutput-WG4uv", "type": "genericNode", "position": {"x": 2418.106177514344, "y": 127.58245532298338}, "data": {"node": {"template": {"_type": "Component", "background_color": {"tool_mode": false, "trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "background_color", "value": "", "display_name": "Background Color", "advanced": true, "input_types": ["Message"], "dynamic": false, "info": "The background color of the icon.", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "chat_icon": {"tool_mode": false, "trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "chat_icon", "value": "", "display_name": "Icon", "advanced": true, "input_types": ["Message"], "dynamic": false, "info": "The icon of the message.", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "code": {"type": "code", "required": true, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "from langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.io import DropdownInput, MessageInput, MessageTextInput, Output\nfrom langflow.schema.message import Message\nfrom langflow.schema.properties import Source\nfrom langflow.utils.constants import (\n    MESSAGE_SENDER_AI,\n    MESSAGE_SENDER_NAME_AI,\n    MESSAGE_SENDER_USER,\n)\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    icon = \"MessagesSquare\"\n    name = \"ChatOutput\"\n    minimized = True\n\n    inputs = [\n        MessageInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Message to be passed as output.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_AI,\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_AI,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n        MessageTextInput(\n            name=\"background_color\",\n            display_name=\"Background Color\",\n            info=\"The background color of the icon.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"chat_icon\",\n            display_name=\"Icon\",\n            info=\"The icon of the message.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"text_color\",\n            display_name=\"Text Color\",\n            info=\"The text color of the name\",\n            advanced=True,\n        ),\n    ]\n    outputs = [\n        Output(\n            display_name=\"Message\",\n            name=\"message\",\n            method=\"message_response\",\n        ),\n    ]\n\n    def _build_source(self, id_: str | None, display_name: str | None, source: str | None) -> Source:\n        source_dict = {}\n        if id_:\n            source_dict[\"id\"] = id_\n        if display_name:\n            source_dict[\"display_name\"] = display_name\n        if source:\n            source_dict[\"source\"] = source\n        return Source(**source_dict)\n\n    async def message_response(self) -> Message:\n        source, icon, display_name, source_id = self.get_properties_from_source_component()\n        background_color = self.background_color\n        text_color = self.text_color\n        if self.chat_icon:\n            icon = self.chat_icon\n        message = self.input_value if isinstance(self.input_value, Message) else Message(text=self.input_value)\n        message.sender = self.sender\n        message.sender_name = self.sender_name\n        message.session_id = self.session_id\n        message.flow_id = self.graph.flow_id if hasattr(self, \"graph\") else None\n        message.properties.source = self._build_source(source_id, display_name, source)\n        message.properties.icon = icon\n        message.properties.background_color = background_color\n        message.properties.text_color = text_color\n        if self.session_id and isinstance(message, Message) and self.should_store_message:\n            stored_message = await self.send_message(\n                message,\n            )\n            self.message.value = stored_message\n            message = stored_message\n\n        self.status = message\n        return message\n", "fileTypes": [], "file_path": "", "password": false, "name": "code", "advanced": true, "dynamic": true, "info": "", "load_from_db": false, "title_case": false}, "data_template": {"tool_mode": false, "trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "data_template", "value": "{text}", "display_name": "Data Template", "advanced": true, "input_types": ["Message"], "dynamic": false, "info": "Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "input_value": {"trace_as_input": true, "tool_mode": false, "trace_as_metadata": true, "load_from_db": false, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "input_value", "value": "", "display_name": "Text", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "Message to be passed as output.", "title_case": false, "type": "str", "_input_type": "MessageInput"}, "sender": {"tool_mode": false, "trace_as_metadata": true, "options": ["Machine", "User"], "options_metadata": [], "combobox": false, "dialog_inputs": {}, "required": false, "placeholder": "", "show": true, "name": "sender", "value": "Machine", "display_name": "Sender Type", "advanced": true, "dynamic": false, "info": "Type of sender.", "title_case": false, "type": "str", "_input_type": "DropdownInput"}, "sender_name": {"tool_mode": false, "trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "sender_name", "value": "AI", "display_name": "Sender Name", "advanced": true, "input_types": ["Message"], "dynamic": false, "info": "Name of the sender.", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "session_id": {"tool_mode": false, "trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "session_id", "value": "", "display_name": "Session ID", "advanced": true, "input_types": ["Message"], "dynamic": false, "info": "The session ID of the chat. If empty, the current session ID parameter will be used.", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "should_store_message": {"tool_mode": false, "trace_as_metadata": true, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "should_store_message", "value": true, "display_name": "Store Messages", "advanced": true, "dynamic": false, "info": "Store the message in the history.", "title_case": false, "type": "bool", "_input_type": "BoolInput"}, "text_color": {"tool_mode": false, "trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "text_color", "value": "", "display_name": "Text Color", "advanced": true, "input_types": ["Message"], "dynamic": false, "info": "The text color of the name", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}}, "description": "Display a chat message in the Playground.", "icon": "MessagesSquare", "base_classes": ["Message"], "display_name": "Chat Output", "documentation": "", "minimized": true, "custom_fields": {}, "output_types": [], "pinned": false, "conditional_paths": [], "frozen": false, "outputs": [{"types": ["Message"], "selected": "Message", "name": "message", "display_name": "Message", "method": "message_response", "value": "__UNDEFINED__", "cache": true, "allows_loop": false}], "field_order": ["input_value", "should_store_message", "sender", "sender_name", "session_id", "data_template", "background_color", "chat_icon", "text_color"], "beta": false, "legacy": false, "edited": false, "metadata": {}, "tool_mode": false, "lf_version": "1.1.3"}, "showNode": true, "type": "ChatOutput", "id": "ChatOutput-WG4uv"}, "selected": false, "measured": {"width": 320, "height": 229}, "dragging": false}, {"id": "File-DO5zV", "type": "genericNode", "position": {"x": -425.8125994932111, "y": 506.1971277087136}, "data": {"node": {"template": {"_type": "Component", "file_path": {"trace_as_metadata": true, "list": true, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "file_path", "value": "", "display_name": "Server File Path", "advanced": true, "input_types": ["Data", "Message"], "dynamic": false, "info": "Data object with a 'file_path' property pointing to server file or a Message object with a path to the file. Supercedes 'Path' but supports same file types.", "title_case": false, "type": "other", "_input_type": "HandleInput"}, "path": {"trace_as_metadata": true, "file_path": "a69ae4ad-71b3-419a-86e9-fb35ad6eadad/2025-03-21_15-28-23_TicketRAG.pdf", "fileTypes": ["txt", "md", "mdx", "csv", "json", "yaml", "yml", "xml", "html", "htm", "pdf", "docx", "py", "sh", "sql", "js", "ts", "tsx", "zip", "tar", "tgz", "bz2", "gz"], "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "path", "value": "", "display_name": "Path", "advanced": false, "dynamic": false, "info": "Supported file extensions: txt, md, mdx, csv, json, yaml, yml, xml, html, htm, pdf, docx, py, sh, sql, js, ts, tsx; optionally bundled in file extensions: zip, tar, tgz, bz2, gz", "title_case": false, "type": "file", "_input_type": "FileInput"}, "code": {"type": "code", "required": true, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "from langflow.base.data import BaseFileComponent\nfrom langflow.base.data.utils import TEXT_FILE_TYPES, parallel_load_data, parse_text_file_to_data\nfrom langflow.io import BoolInput, IntInput\nfrom langflow.schema import Data\n\n\nclass FileComponent(BaseFileComponent):\n    \"\"\"Handles loading and processing of individual or zipped text files.\n\n    This component supports processing multiple valid files within a zip archive,\n    resolving paths, validating file types, and optionally using multithreading for processing.\n    \"\"\"\n\n    display_name = \"File\"\n    description = \"Load a file to be used in your project.\"\n    icon = \"file-text\"\n    name = \"File\"\n\n    VALID_EXTENSIONS = TEXT_FILE_TYPES\n\n    inputs = [\n        *BaseFileComponent._base_inputs,\n        BoolInput(\n            name=\"use_multithreading\",\n            display_name=\"[Deprecated] Use Multithreading\",\n            advanced=True,\n            value=True,\n            info=\"Set 'Processing Concurrency' greater than 1 to enable multithreading.\",\n        ),\n        IntInput(\n            name=\"concurrency_multithreading\",\n            display_name=\"Processing Concurrency\",\n            advanced=True,\n            info=\"When multiple files are being processed, the number of files to process concurrently.\",\n            value=1,\n        ),\n    ]\n\n    outputs = [\n        *BaseFileComponent._base_outputs,\n    ]\n\n    def process_files(self, file_list: list[BaseFileComponent.BaseFile]) -> list[BaseFileComponent.BaseFile]:\n        \"\"\"Processes files either sequentially or in parallel, depending on concurrency settings.\n\n        Args:\n            file_list (list[BaseFileComponent.BaseFile]): List of files to process.\n\n        Returns:\n            list[BaseFileComponent.BaseFile]: Updated list of files with merged data.\n        \"\"\"\n\n        def process_file(file_path: str, *, silent_errors: bool = False) -> Data | None:\n            \"\"\"Processes a single file and returns its Data object.\"\"\"\n            try:\n                return parse_text_file_to_data(file_path, silent_errors=silent_errors)\n            except FileNotFoundError as e:\n                msg = f\"File not found: {file_path}. Error: {e}\"\n                self.log(msg)\n                if not silent_errors:\n                    raise\n                return None\n            except Exception as e:\n                msg = f\"Unexpected error processing {file_path}: {e}\"\n                self.log(msg)\n                if not silent_errors:\n                    raise\n                return None\n\n        if not file_list:\n            msg = \"No files to process.\"\n            raise ValueError(msg)\n\n        concurrency = 1 if not self.use_multithreading else max(1, self.concurrency_multithreading)\n        file_count = len(file_list)\n\n        parallel_processing_threshold = 2\n        if concurrency < parallel_processing_threshold or file_count < parallel_processing_threshold:\n            if file_count > 1:\n                self.log(f\"Processing {file_count} files sequentially.\")\n            processed_data = [process_file(str(file.path), silent_errors=self.silent_errors) for file in file_list]\n        else:\n            self.log(f\"Starting parallel processing of {file_count} files with concurrency: {concurrency}.\")\n            file_paths = [str(file.path) for file in file_list]\n            processed_data = parallel_load_data(\n                file_paths,\n                silent_errors=self.silent_errors,\n                load_function=process_file,\n                max_concurrency=concurrency,\n            )\n\n        # Use rollup_basefile_data to merge processed data with BaseFile objects\n        return self.rollup_data(file_list, processed_data)\n", "fileTypes": [], "file_path": "", "password": false, "name": "code", "advanced": true, "dynamic": true, "info": "", "load_from_db": false, "title_case": false}, "concurrency_multithreading": {"tool_mode": false, "trace_as_metadata": true, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "concurrency_multithreading", "value": 1, "display_name": "Processing Concurrency", "advanced": true, "dynamic": false, "info": "When multiple files are being processed, the number of files to process concurrently.", "title_case": false, "type": "int", "_input_type": "IntInput"}, "delete_server_file_after_processing": {"tool_mode": false, "trace_as_metadata": true, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "delete_server_file_after_processing", "value": true, "display_name": "Delete Server File After Processing", "advanced": true, "dynamic": false, "info": "If true, the Server File Path will be deleted after processing.", "title_case": false, "type": "bool", "_input_type": "BoolInput"}, "ignore_unspecified_files": {"tool_mode": false, "trace_as_metadata": true, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "ignore_unspecified_files", "value": false, "display_name": "Ignore Unspecified Files", "advanced": true, "dynamic": false, "info": "If true, Data with no 'file_path' property will be ignored.", "title_case": false, "type": "bool", "_input_type": "BoolInput"}, "ignore_unsupported_extensions": {"tool_mode": false, "trace_as_metadata": true, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "ignore_unsupported_extensions", "value": true, "display_name": "Ignore Unsupported Extensions", "advanced": true, "dynamic": false, "info": "If true, files with unsupported extensions will not be processed.", "title_case": false, "type": "bool", "_input_type": "BoolInput"}, "silent_errors": {"tool_mode": false, "trace_as_metadata": true, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "silent_errors", "value": false, "display_name": "Silent Errors", "advanced": true, "dynamic": false, "info": "If true, errors will not raise an exception.", "title_case": false, "type": "bool", "_input_type": "BoolInput"}, "use_multithreading": {"tool_mode": false, "trace_as_metadata": true, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "use_multithreading", "value": true, "display_name": "[Deprecated] Use Multithreading", "advanced": true, "dynamic": false, "info": "Set 'Processing Concurrency' greater than 1 to enable multithreading.", "title_case": false, "type": "bool", "_input_type": "BoolInput"}}, "description": "Load a file to be used in your project.", "icon": "file-text", "base_classes": ["Data"], "display_name": "File", "documentation": "", "minimized": false, "custom_fields": {}, "output_types": [], "pinned": false, "conditional_paths": [], "frozen": false, "outputs": [{"types": ["Data"], "selected": "Data", "name": "data", "display_name": "Data", "method": "load_files", "value": "__UNDEFINED__", "cache": true, "required_inputs": [], "allows_loop": false, "tool_mode": true}], "field_order": ["path", "file_path", "silent_errors", "delete_server_file_after_processing", "ignore_unsupported_extensions", "ignore_unspecified_files", "use_multithreading", "concurrency_multithreading"], "beta": false, "legacy": false, "edited": false, "metadata": {}, "tool_mode": false, "category": "data", "key": "File", "score": 9.159206968830713e-17, "lf_version": "1.1.3"}, "showNode": true, "type": "File", "id": "File-DO5zV"}, "selected": false, "measured": {"width": 320, "height": 227}, "dragging": false}, {"id": "SplitText-RnCyU", "type": "genericNode", "position": {"x": 10.920670920594745, "y": 489.3996942312597}, "data": {"node": {"template": {"_type": "Component", "data_inputs": {"trace_as_metadata": true, "list": true, "list_add_label": "Add More", "required": true, "placeholder": "", "show": true, "name": "data_inputs", "value": "", "display_name": "Data Inputs", "advanced": false, "input_types": ["Data"], "dynamic": false, "info": "The data to split.", "title_case": false, "type": "other", "_input_type": "HandleInput"}, "chunk_overlap": {"tool_mode": false, "trace_as_metadata": true, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "chunk_overlap", "value": 200, "display_name": "Chunk Overlap", "advanced": false, "dynamic": false, "info": "Number of characters to overlap between chunks.", "title_case": false, "type": "int", "_input_type": "IntInput"}, "chunk_size": {"tool_mode": false, "trace_as_metadata": true, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "chunk_size", "value": 1000, "display_name": "Chunk Size", "advanced": false, "dynamic": false, "info": "The maximum number of characters in each chunk.", "title_case": false, "type": "int", "_input_type": "IntInput"}, "code": {"type": "code", "required": true, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "from langchain_text_splitters import CharacterTextSplitter\n\nfrom langflow.custom import Component\nfrom langflow.io import HandleInput, IntInput, MessageTextInput, Output\nfrom langflow.schema import Data, DataFrame\nfrom langflow.utils.util import unescape_string\n\n\nclass SplitTextComponent(Component):\n    display_name: str = \"Split Text\"\n    description: str = \"Split text into chunks based on specified criteria.\"\n    icon = \"scissors-line-dashed\"\n    name = \"SplitText\"\n\n    inputs = [\n        HandleInput(\n            name=\"data_inputs\",\n            display_name=\"Data Inputs\",\n            info=\"The data to split.\",\n            input_types=[\"Data\"],\n            is_list=True,\n            required=True,\n        ),\n        IntInput(\n            name=\"chunk_overlap\",\n            display_name=\"Chunk Overlap\",\n            info=\"Number of characters to overlap between chunks.\",\n            value=200,\n        ),\n        IntInput(\n            name=\"chunk_size\",\n            display_name=\"Chunk Size\",\n            info=\"The maximum number of characters in each chunk.\",\n            value=1000,\n        ),\n        MessageTextInput(\n            name=\"separator\",\n            display_name=\"Separator\",\n            info=\"The character to split on. Defaults to newline.\",\n            value=\"\\n\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Chunks\", name=\"chunks\", method=\"split_text\"),\n        Output(display_name=\"DataFrame\", name=\"dataframe\", method=\"as_dataframe\"),\n    ]\n\n    def _docs_to_data(self, docs):\n        return [Data(text=doc.page_content, data=doc.metadata) for doc in docs]\n\n    def split_text(self) -> list[Data]:\n        separator = unescape_string(self.separator)\n\n        documents = [_input.to_lc_document() for _input in self.data_inputs if isinstance(_input, Data)]\n\n        splitter = CharacterTextSplitter(\n            chunk_overlap=self.chunk_overlap,\n            chunk_size=self.chunk_size,\n            separator=separator,\n        )\n        docs = splitter.split_documents(documents)\n        data = self._docs_to_data(docs)\n        self.status = data\n        return data\n\n    def as_dataframe(self) -> DataFrame:\n        return DataFrame(self.split_text())\n", "fileTypes": [], "file_path": "", "password": false, "name": "code", "advanced": true, "dynamic": true, "info": "", "load_from_db": false, "title_case": false}, "separator": {"tool_mode": false, "trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "separator", "value": "\n", "display_name": "Separator", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "The character to split on. Defaults to newline.", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}}, "description": "Split text into chunks based on specified criteria.", "icon": "scissors-line-dashed", "base_classes": ["Data", "DataFrame"], "display_name": "Split Text", "documentation": "", "minimized": false, "custom_fields": {}, "output_types": [], "pinned": false, "conditional_paths": [], "frozen": false, "outputs": [{"types": ["Data"], "selected": "Data", "name": "chunks", "display_name": "Chunks", "method": "split_text", "value": "__UNDEFINED__", "cache": true, "allows_loop": false, "tool_mode": true}, {"types": ["DataFrame"], "selected": "DataFrame", "name": "dataframe", "display_name": "DataFrame", "method": "as_dataframe", "value": "__UNDEFINED__", "cache": true, "allows_loop": false, "tool_mode": true}], "field_order": ["data_inputs", "chunk_overlap", "chunk_size", "separator"], "beta": false, "legacy": false, "edited": false, "metadata": {}, "tool_mode": false, "category": "processing", "key": "SplitText", "score": 0.0006561452663029057, "lf_version": "1.1.3"}, "showNode": true, "type": "SplitText", "id": "SplitText-RnCyU"}, "selected": false, "measured": {"width": 320, "height": 505}, "dragging": false}, {"id": "HuggingFaceInferenceAPIEmbeddings-Suz5I", "type": "genericNode", "position": {"x": 10.920670920594716, "y": 1098.90656612745}, "data": {"node": {"template": {"_type": "Component", "api_key": {"load_from_db": false, "required": false, "placeholder": "", "show": true, "name": "api_key", "value": null, "display_name": "API Key", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "Required for non-local inference endpoints. Local inference does not require an API Key.", "title_case": false, "password": true, "type": "str", "_input_type": "SecretStrInput"}, "code": {"type": "code", "required": true, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "from urllib.parse import urlparse\n\nimport requests\nfrom langchain_community.embeddings.huggingface import HuggingFaceInferenceAPIEmbeddings\n\n# Next update: use langchain_huggingface\nfrom pydantic import SecretStr\nfrom tenacity import retry, stop_after_attempt, wait_fixed\n\nfrom langflow.base.embeddings.model import LCEmbeddingsModel\nfrom langflow.field_typing import Embeddings\nfrom langflow.io import MessageTextInput, Output, SecretStrInput\n\n\nclass HuggingFaceInferenceAPIEmbeddingsComponent(LCEmbeddingsModel):\n    display_name = \"HuggingFace Embeddings Inference\"\n    description = \"Generate embeddings using HuggingFace Text Embeddings Inference (TEI)\"\n    documentation = \"https://huggingface.co/docs/text-embeddings-inference/index\"\n    icon = \"HuggingFace\"\n    name = \"HuggingFaceInferenceAPIEmbeddings\"\n\n    inputs = [\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"API Key\",\n            advanced=False,\n            info=\"Required for non-local inference endpoints. Local inference does not require an API Key.\",\n        ),\n        MessageTextInput(\n            name=\"inference_endpoint\",\n            display_name=\"Inference Endpoint\",\n            required=True,\n            value=\"https://api-inference.huggingface.co/models/\",\n            info=\"Custom inference endpoint URL.\",\n        ),\n        MessageTextInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            value=\"BAAI/bge-large-en-v1.5\",\n            info=\"The name of the model to use for text embeddings.\",\n            required=True,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Embeddings\", name=\"embeddings\", method=\"build_embeddings\"),\n    ]\n\n    def validate_inference_endpoint(self, inference_endpoint: str) -> bool:\n        parsed_url = urlparse(inference_endpoint)\n        if not all([parsed_url.scheme, parsed_url.netloc]):\n            msg = (\n                f\"Invalid inference endpoint format: '{self.inference_endpoint}'. \"\n                \"Please ensure the URL includes both a scheme (e.g., 'http://' or 'https://') and a domain name. \"\n                \"Example: 'http://localhost:8080' or 'https://api.example.com'\"\n            )\n            raise ValueError(msg)\n\n        try:\n            response = requests.get(f\"{inference_endpoint}/health\", timeout=5)\n        except requests.RequestException as e:\n            msg = (\n                f\"Inference endpoint '{inference_endpoint}' is not responding. \"\n                \"Please ensure the URL is correct and the service is running.\"\n            )\n            raise ValueError(msg) from e\n\n        if response.status_code != requests.codes.ok:\n            msg = f\"HuggingFace health check failed: {response.status_code}\"\n            raise ValueError(msg)\n        # returning True to solve linting error\n        return True\n\n    def get_api_url(self) -> str:\n        if \"huggingface\" in self.inference_endpoint.lower():\n            return f\"{self.inference_endpoint}\"\n        return self.inference_endpoint\n\n    @retry(stop=stop_after_attempt(3), wait=wait_fixed(2))\n    def create_huggingface_embeddings(\n        self, api_key: SecretStr, api_url: str, model_name: str\n    ) -> HuggingFaceInferenceAPIEmbeddings:\n        return HuggingFaceInferenceAPIEmbeddings(api_key=api_key, api_url=api_url, model_name=model_name)\n\n    def build_embeddings(self) -> Embeddings:\n        api_url = self.get_api_url()\n\n        is_local_url = (\n            api_url.startswith((\"http://localhost\", \"http://127.0.0.1\", \"http://0.0.0.0\", \"http://docker\"))\n            or \"huggingface.co\" not in api_url.lower()\n        )\n\n        if not self.api_key and is_local_url:\n            self.validate_inference_endpoint(api_url)\n            api_key = SecretStr(\"APIKeyForLocalDeployment\")\n        elif not self.api_key:\n            msg = \"API Key is required for non-local inference endpoints\"\n            raise ValueError(msg)\n        else:\n            api_key = SecretStr(self.api_key).get_secret_value()\n\n        try:\n            return self.create_huggingface_embeddings(api_key, api_url, self.model_name)\n        except Exception as e:\n            msg = \"Could not connect to HuggingFace Inference API.\"\n            raise ValueError(msg) from e\n", "fileTypes": [], "file_path": "", "password": false, "name": "code", "advanced": true, "dynamic": true, "info": "", "load_from_db": false, "title_case": false}, "inference_endpoint": {"tool_mode": false, "trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "list_add_label": "Add More", "required": true, "placeholder": "", "show": true, "name": "inference_endpoint", "value": "http://localhost:8080", "display_name": "Inference Endpoint", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "Custom inference endpoint URL.", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "model_name": {"tool_mode": false, "trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "list_add_label": "Add More", "required": true, "placeholder": "", "show": true, "name": "model_name", "value": "BAAI/bge-large-en-v1.5", "display_name": "Model Name", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "The name of the model to use for text embeddings.", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}}, "description": "Generate embeddings using HuggingFace Text Embeddings Inference (TEI)", "icon": "HuggingFace", "base_classes": ["Embeddings"], "display_name": "HuggingFace Embeddings Inference", "documentation": "https://huggingface.co/docs/text-embeddings-inference/index", "minimized": false, "custom_fields": {}, "output_types": [], "pinned": false, "conditional_paths": [], "frozen": false, "outputs": [{"types": ["Embeddings"], "selected": "Embeddings", "name": "embeddings", "hidden": null, "display_name": "Embeddings", "method": "build_embeddings", "value": "__UNDEFINED__", "cache": true, "required_inputs": null, "allows_loop": false, "tool_mode": true}], "field_order": ["api_key", "inference_endpoint", "model_name"], "beta": false, "legacy": false, "edited": false, "metadata": {}, "tool_mode": false}, "showNode": true, "type": "HuggingFaceInferenceAPIEmbeddings", "id": "HuggingFaceInferenceAPIEmbeddings-Suz5I"}, "selected": false, "measured": {"width": 320, "height": 413}, "dragging": false}, {"id": "FAISS-toNtm", "type": "genericNode", "position": {"x": 505.2451418285288, "y": 743.7608297469926}, "data": {"node": {"template": {"_type": "Component", "embedding": {"trace_as_metadata": true, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "embedding", "value": "", "display_name": "Embedding", "advanced": false, "input_types": ["Embeddings"], "dynamic": false, "info": "", "title_case": false, "type": "other", "_input_type": "HandleInput"}, "ingest_data": {"tool_mode": false, "trace_as_metadata": true, "list": false, "list_add_label": "Add More", "trace_as_input": true, "required": false, "placeholder": "", "show": true, "name": "ingest_data", "value": "", "display_name": "Ingest Data", "advanced": false, "input_types": ["Data"], "dynamic": false, "info": "", "title_case": false, "type": "other", "_input_type": "DataInput"}, "allow_dangerous_deserialization": {"tool_mode": false, "trace_as_metadata": true, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "allow_dangerous_deserialization", "value": true, "display_name": "Allow Dangerous Deserialization", "advanced": true, "dynamic": false, "info": "Set to True to allow loading pickle files from untrusted sources. Only enable this if you trust the source of the data.", "title_case": false, "type": "bool", "_input_type": "BoolInput"}, "code": {"type": "code", "required": true, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "from langchain_community.vectorstores import FAISS\n\nfrom langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store\nfrom langflow.helpers.data import docs_to_data\nfrom langflow.io import BoolInput, HandleInput, IntInput, StrInput\nfrom langflow.schema import Data\n\n\nclass FaissVectorStoreComponent(LCVectorStoreComponent):\n    \"\"\"FAISS Vector Store with search capabilities.\"\"\"\n\n    display_name: str = \"FAISS\"\n    description: str = \"FAISS Vector Store with search capabilities\"\n    name = \"FAISS\"\n    icon = \"FAISS\"\n\n    inputs = [\n        StrInput(\n            name=\"index_name\",\n            display_name=\"Index Name\",\n            value=\"langflow_index\",\n        ),\n        StrInput(\n            name=\"persist_directory\",\n            display_name=\"Persist Directory\",\n            info=\"Path to save the FAISS index. It will be relative to where Langflow is running.\",\n        ),\n        *LCVectorStoreComponent.inputs,\n        BoolInput(\n            name=\"allow_dangerous_deserialization\",\n            display_name=\"Allow Dangerous Deserialization\",\n            info=\"Set to True to allow loading pickle files from untrusted sources. \"\n            \"Only enable this if you trust the source of the data.\",\n            advanced=True,\n            value=True,\n        ),\n        HandleInput(name=\"embedding\", display_name=\"Embedding\", input_types=[\"Embeddings\"]),\n        IntInput(\n            name=\"number_of_results\",\n            display_name=\"Number of Results\",\n            info=\"Number of results to return.\",\n            advanced=True,\n            value=4,\n        ),\n    ]\n\n    @check_cached_vector_store\n    def build_vector_store(self) -> FAISS:\n        \"\"\"Builds the FAISS object.\"\"\"\n        if not self.persist_directory:\n            msg = \"Folder path is required to save the FAISS index.\"\n            raise ValueError(msg)\n        path = self.resolve_path(self.persist_directory)\n        \n        # Ensure the directory exists\n        from pathlib import Path\n        path_obj = Path(path)\n        path_obj.mkdir(parents=True, exist_ok=True)\n        \n        # Clear existing index files to overwrite old data\n        index_file = path_obj / f\"{self.index_name}.faiss\"\n        pickle_file = path_obj / f\"{self.index_name}.pkl\"\n        if index_file.exists():\n            index_file.unlink()\n        if pickle_file.exists():\n            pickle_file.unlink()\n\n        documents = []\n\n        for _input in self.ingest_data or []:\n            if isinstance(_input, Data):\n                documents.append(_input.to_lc_document())\n            else:\n                documents.append(_input)\n\n        faiss = FAISS.from_documents(documents=documents, embedding=self.embedding)\n        faiss.save_local(str(path), self.index_name)\n\n        return faiss\n\n    def search_documents(self) -> list[Data]:\n        \"\"\"Search for documents in the FAISS vector store.\"\"\"\n        if not self.persist_directory:\n            msg = \"Folder path is required to load the FAISS index.\"\n            raise ValueError(msg)\n        path = self.resolve_path(self.persist_directory)\n\n        # Always rebuild the vector store with the latest data instead of loading\n        vector_store = self.build_vector_store()\n\n        if not vector_store:\n            msg = \"Failed to load the FAISS index.\"\n            raise ValueError(msg)\n\n        self.log(f\"Search input: {self.search_query}\")\n        self.log(f\"Number of results: {self.number_of_results}\")\n\n        if self.search_query and isinstance(self.search_query, str) and self.search_query.strip():\n            docs = vector_store.similarity_search(\n                query=self.search_query,\n                k=self.number_of_results,\n            )\n\n            self.log(f\"Retrieved documents: {len(docs)}\")\n\n            data = docs_to_data(docs)\n            self.log(f\"Converted documents to data: {len(data)}\")\n            self.log(data)\n            return data  # Return the search results data\n        self.log(\"No search input provided. Skipping search.\")\n        return []", "fileTypes": [], "file_path": "", "password": false, "name": "code", "advanced": true, "dynamic": true, "info": "", "load_from_db": false, "title_case": false}, "index_name": {"tool_mode": false, "trace_as_metadata": true, "load_from_db": false, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "index_name", "value": "langflow_index", "display_name": "Index Name", "advanced": false, "dynamic": false, "info": "", "title_case": false, "type": "str", "_input_type": "StrInput"}, "number_of_results": {"tool_mode": false, "trace_as_metadata": true, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "number_of_results", "value": 4, "display_name": "Number of Results", "advanced": true, "dynamic": false, "info": "Number of results to return.", "title_case": false, "type": "int", "_input_type": "IntInput"}, "persist_directory": {"tool_mode": false, "trace_as_metadata": true, "load_from_db": false, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "persist_directory", "value": "/tmp", "display_name": "Persist Directory", "advanced": false, "dynamic": false, "info": "Path to save the FAISS index. It will be relative to where Langflow is running.", "title_case": false, "type": "str", "_input_type": "StrInput"}, "search_query": {"tool_mode": true, "trace_as_input": true, "multiline": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "search_query", "value": "", "display_name": "Search Query", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "", "title_case": false, "type": "str", "_input_type": "MultilineInput"}}, "description": "FAISS Vector Store with search capabilities", "icon": "FAISS", "base_classes": ["Data", "DataFrame"], "display_name": "FAISS", "documentation": "", "minimized": false, "custom_fields": {}, "output_types": [], "pinned": false, "conditional_paths": [], "frozen": false, "outputs": [{"types": ["Data"], "selected": "Data", "name": "search_results", "hidden": null, "display_name": "Search Results", "method": "search_documents", "value": "__UNDEFINED__", "cache": true, "required_inputs": [], "allows_loop": false, "tool_mode": true}, {"types": ["DataFrame"], "selected": "DataFrame", "name": "dataframe", "hidden": null, "display_name": "DataFrame", "method": "as_dataframe", "value": "__UNDEFINED__", "cache": true, "required_inputs": [], "allows_loop": false, "tool_mode": true}], "field_order": ["index_name", "persist_directory", "ingest_data", "search_query", "allow_dangerous_deserialization", "embedding", "number_of_results"], "beta": false, "legacy": false, "edited": true, "metadata": {}, "tool_mode": false, "lf_version": "1.1.3"}, "showNode": true, "type": "FAISS", "id": "FAISS-toNtm"}, "selected": false, "measured": {"width": 320, "height": 529}, "dragging": false}, {"id": "HuggingFaceInferenceAPIEmbeddings-JqfkL", "type": "genericNode", "position": {"x": 1718.3970179738183, "y": 1092.2660479292863}, "data": {"node": {"template": {"_type": "Component", "api_key": {"load_from_db": false, "required": false, "placeholder": "", "show": true, "name": "api_key", "value": null, "display_name": "API Key", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "Required for non-local inference endpoints. Local inference does not require an API Key.", "title_case": false, "password": true, "type": "str", "_input_type": "SecretStrInput"}, "code": {"type": "code", "required": true, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "from urllib.parse import urlparse\n\nimport requests\nfrom langchain_community.embeddings.huggingface import HuggingFaceInferenceAPIEmbeddings\n\n# Next update: use langchain_huggingface\nfrom pydantic import SecretStr\nfrom tenacity import retry, stop_after_attempt, wait_fixed\n\nfrom langflow.base.embeddings.model import LCEmbeddingsModel\nfrom langflow.field_typing import Embeddings\nfrom langflow.io import MessageTextInput, Output, SecretStrInput\n\n\nclass HuggingFaceInferenceAPIEmbeddingsComponent(LCEmbeddingsModel):\n    display_name = \"HuggingFace Embeddings Inference\"\n    description = \"Generate embeddings using HuggingFace Text Embeddings Inference (TEI)\"\n    documentation = \"https://huggingface.co/docs/text-embeddings-inference/index\"\n    icon = \"HuggingFace\"\n    name = \"HuggingFaceInferenceAPIEmbeddings\"\n\n    inputs = [\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"API Key\",\n            advanced=False,\n            info=\"Required for non-local inference endpoints. Local inference does not require an API Key.\",\n        ),\n        MessageTextInput(\n            name=\"inference_endpoint\",\n            display_name=\"Inference Endpoint\",\n            required=True,\n            value=\"https://api-inference.huggingface.co/models/\",\n            info=\"Custom inference endpoint URL.\",\n        ),\n        MessageTextInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            value=\"BAAI/bge-large-en-v1.5\",\n            info=\"The name of the model to use for text embeddings.\",\n            required=True,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Embeddings\", name=\"embeddings\", method=\"build_embeddings\"),\n    ]\n\n    def validate_inference_endpoint(self, inference_endpoint: str) -> bool:\n        parsed_url = urlparse(inference_endpoint)\n        if not all([parsed_url.scheme, parsed_url.netloc]):\n            msg = (\n                f\"Invalid inference endpoint format: '{self.inference_endpoint}'. \"\n                \"Please ensure the URL includes both a scheme (e.g., 'http://' or 'https://') and a domain name. \"\n                \"Example: 'http://localhost:8080' or 'https://api.example.com'\"\n            )\n            raise ValueError(msg)\n\n        try:\n            response = requests.get(f\"{inference_endpoint}/health\", timeout=5)\n        except requests.RequestException as e:\n            msg = (\n                f\"Inference endpoint '{inference_endpoint}' is not responding. \"\n                \"Please ensure the URL is correct and the service is running.\"\n            )\n            raise ValueError(msg) from e\n\n        if response.status_code != requests.codes.ok:\n            msg = f\"HuggingFace health check failed: {response.status_code}\"\n            raise ValueError(msg)\n        # returning True to solve linting error\n        return True\n\n    def get_api_url(self) -> str:\n        if \"huggingface\" in self.inference_endpoint.lower():\n            return f\"{self.inference_endpoint}\"\n        return self.inference_endpoint\n\n    @retry(stop=stop_after_attempt(3), wait=wait_fixed(2))\n    def create_huggingface_embeddings(\n        self, api_key: SecretStr, api_url: str, model_name: str\n    ) -> HuggingFaceInferenceAPIEmbeddings:\n        return HuggingFaceInferenceAPIEmbeddings(api_key=api_key, api_url=api_url, model_name=model_name)\n\n    def build_embeddings(self) -> Embeddings:\n        api_url = self.get_api_url()\n\n        is_local_url = (\n            api_url.startswith((\"http://localhost\", \"http://127.0.0.1\", \"http://0.0.0.0\", \"http://docker\"))\n            or \"huggingface.co\" not in api_url.lower()\n        )\n\n        if not self.api_key and is_local_url:\n            self.validate_inference_endpoint(api_url)\n            api_key = SecretStr(\"APIKeyForLocalDeployment\")\n        elif not self.api_key:\n            msg = \"API Key is required for non-local inference endpoints\"\n            raise ValueError(msg)\n        else:\n            api_key = SecretStr(self.api_key).get_secret_value()\n\n        try:\n            return self.create_huggingface_embeddings(api_key, api_url, self.model_name)\n        except Exception as e:\n            msg = \"Could not connect to HuggingFace Inference API.\"\n            raise ValueError(msg) from e\n", "fileTypes": [], "file_path": "", "password": false, "name": "code", "advanced": true, "dynamic": true, "info": "", "load_from_db": false, "title_case": false}, "inference_endpoint": {"tool_mode": false, "trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "list_add_label": "Add More", "required": true, "placeholder": "", "show": true, "name": "inference_endpoint", "value": "http://localhost:8080", "display_name": "Inference Endpoint", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "Custom inference endpoint URL.", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "model_name": {"tool_mode": false, "trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "list_add_label": "Add More", "required": true, "placeholder": "", "show": true, "name": "model_name", "value": "BAAI/bge-large-en-v1.5", "display_name": "Model Name", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "The name of the model to use for text embeddings.", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}}, "description": "Generate embeddings using HuggingFace Text Embeddings Inference (TEI)", "icon": "HuggingFace", "base_classes": ["Embeddings"], "display_name": "HuggingFace Embeddings Inference", "documentation": "https://huggingface.co/docs/text-embeddings-inference/index", "minimized": false, "custom_fields": {}, "output_types": [], "pinned": false, "conditional_paths": [], "frozen": false, "outputs": [{"types": ["Embeddings"], "selected": "Embeddings", "name": "embeddings", "hidden": null, "display_name": "Embeddings", "method": "build_embeddings", "value": "__UNDEFINED__", "cache": true, "required_inputs": null, "allows_loop": false, "tool_mode": true}], "field_order": ["api_key", "inference_endpoint", "model_name"], "beta": false, "legacy": false, "edited": false, "metadata": {}, "tool_mode": false}, "showNode": true, "type": "HuggingFaceInferenceAPIEmbeddings", "id": "HuggingFaceInferenceAPIEmbeddings-JqfkL"}, "selected": false, "measured": {"width": 320, "height": 413}, "dragging": false}, {"id": "FAISS-SqfVh", "type": "genericNode", "position": {"x": 2098.3372516279587, "y": 516.3343847893979}, "data": {"node": {"template": {"_type": "Component", "embedding": {"trace_as_metadata": true, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "embedding", "value": "", "display_name": "Embedding", "advanced": false, "input_types": ["Embeddings"], "dynamic": false, "info": "", "title_case": false, "type": "other", "_input_type": "HandleInput"}, "ingest_data": {"tool_mode": false, "trace_as_metadata": true, "list": false, "list_add_label": "Add More", "trace_as_input": true, "required": false, "placeholder": "", "show": true, "name": "ingest_data", "value": "", "display_name": "Ingest Data", "advanced": false, "input_types": ["Data"], "dynamic": false, "info": "", "title_case": false, "type": "other", "_input_type": "DataInput"}, "allow_dangerous_deserialization": {"tool_mode": false, "trace_as_metadata": true, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "allow_dangerous_deserialization", "value": true, "display_name": "Allow Dangerous Deserialization", "advanced": true, "dynamic": false, "info": "Set to True to allow loading pickle files from untrusted sources. Only enable this if you trust the source of the data.", "title_case": false, "type": "bool", "_input_type": "BoolInput"}, "code": {"type": "code", "required": true, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "from langchain_community.vectorstores import FAISS\n\nfrom langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store\nfrom langflow.helpers.data import docs_to_data\nfrom langflow.io import BoolInput, HandleInput, IntInput, StrInput\nfrom langflow.schema import Data\n\n\nclass FaissVectorStoreComponent(LCVectorStoreComponent):\n    \"\"\"FAISS Vector Store with search capabilities.\"\"\"\n\n    display_name: str = \"FAISS\"\n    description: str = \"FAISS Vector Store with search capabilities\"\n    name = \"FAISS\"\n    icon = \"FAISS\"\n\n    inputs = [\n        StrInput(\n            name=\"index_name\",\n            display_name=\"Index Name\",\n            value=\"langflow_index\",\n        ),\n        StrInput(\n            name=\"persist_directory\",\n            display_name=\"Persist Directory\",\n            info=\"Path to save the FAISS index. It will be relative to where Langflow is running.\",\n        ),\n        *LCVectorStoreComponent.inputs,\n        BoolInput(\n            name=\"allow_dangerous_deserialization\",\n            display_name=\"Allow Dangerous Deserialization\",\n            info=\"Set to True to allow loading pickle files from untrusted sources. \"\n            \"Only enable this if you trust the source of the data.\",\n            advanced=True,\n            value=True,\n        ),\n        HandleInput(name=\"embedding\", display_name=\"Embedding\", input_types=[\"Embeddings\"]),\n        IntInput(\n            name=\"number_of_results\",\n            display_name=\"Number of Results\",\n            info=\"Number of results to return.\",\n            advanced=True,\n            value=4,\n        ),\n    ]\n\n    @check_cached_vector_store\n    def build_vector_store(self) -> FAISS:\n        \"\"\"Builds the FAISS object.\"\"\"\n        if not self.persist_directory:\n            msg = \"Folder path is required to save the FAISS index.\"\n            raise ValueError(msg)\n        path = self.resolve_path(self.persist_directory)\n        \n        # Ensure the directory exists\n        from pathlib import Path\n        path_obj = Path(path)\n        path_obj.mkdir(parents=True, exist_ok=True)\n        \n        # Clear existing index files to overwrite old data\n        index_file = path_obj / f\"{self.index_name}.faiss\"\n        pickle_file = path_obj / f\"{self.index_name}.pkl\"\n        if index_file.exists():\n            index_file.unlink()\n        if pickle_file.exists():\n            pickle_file.unlink()\n\n        documents = []\n\n        for _input in self.ingest_data or []:\n            if isinstance(_input, Data):\n                documents.append(_input.to_lc_document())\n            else:\n                documents.append(_input)\n\n        faiss = FAISS.from_documents(documents=documents, embedding=self.embedding)\n        faiss.save_local(str(path), self.index_name)\n\n        return faiss\n\n    def search_documents(self) -> list[Data]:\n        \"\"\"Search for documents in the FAISS vector store.\"\"\"\n        if not self.persist_directory:\n            msg = \"Folder path is required to load the FAISS index.\"\n            raise ValueError(msg)\n        path = self.resolve_path(self.persist_directory)\n\n        # Always rebuild the vector store with the latest data instead of loading\n        vector_store = self.build_vector_store()\n\n        if not vector_store:\n            msg = \"Failed to load the FAISS index.\"\n            raise ValueError(msg)\n\n        self.log(f\"Search input: {self.search_query}\")\n        self.log(f\"Number of results: {self.number_of_results}\")\n\n        if self.search_query and isinstance(self.search_query, str) and self.search_query.strip():\n            docs = vector_store.similarity_search(\n                query=self.search_query,\n                k=self.number_of_results,\n            )\n\n            self.log(f\"Retrieved documents: {len(docs)}\")\n\n            data = docs_to_data(docs)\n            self.log(f\"Converted documents to data: {len(data)}\")\n            self.log(data)\n            return data  # Return the search results data\n        self.log(\"No search input provided. Skipping search.\")\n        return []", "fileTypes": [], "file_path": "", "password": false, "name": "code", "advanced": true, "dynamic": true, "info": "", "load_from_db": false, "title_case": false}, "index_name": {"tool_mode": false, "trace_as_metadata": true, "load_from_db": false, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "index_name", "value": "langflow_index", "display_name": "Index Name", "advanced": false, "dynamic": false, "info": "", "title_case": false, "type": "str", "_input_type": "StrInput"}, "number_of_results": {"tool_mode": false, "trace_as_metadata": true, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "number_of_results", "value": 4, "display_name": "Number of Results", "advanced": true, "dynamic": false, "info": "Number of results to return.", "title_case": false, "type": "int", "_input_type": "IntInput"}, "persist_directory": {"tool_mode": false, "trace_as_metadata": true, "load_from_db": false, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "persist_directory", "value": "/tmp", "display_name": "Persist Directory", "advanced": false, "dynamic": false, "info": "Path to save the FAISS index. It will be relative to where Langflow is running.", "title_case": false, "type": "str", "_input_type": "StrInput"}, "search_query": {"tool_mode": true, "trace_as_input": true, "multiline": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "search_query", "value": "", "display_name": "Search Query", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "", "title_case": false, "type": "str", "_input_type": "MultilineInput"}}, "description": "FAISS Vector Store with search capabilities", "icon": "FAISS", "base_classes": ["Data", "DataFrame"], "display_name": "FAISS", "documentation": "", "minimized": false, "custom_fields": {}, "output_types": [], "pinned": false, "conditional_paths": [], "frozen": false, "outputs": [{"types": ["Data"], "selected": "Data", "name": "search_results", "hidden": null, "display_name": "Search Results", "method": "search_documents", "value": "__UNDEFINED__", "cache": true, "required_inputs": [], "allows_loop": false, "tool_mode": true}, {"types": ["DataFrame"], "selected": "DataFrame", "name": "dataframe", "hidden": null, "display_name": "DataFrame", "method": "as_dataframe", "value": "__UNDEFINED__", "cache": true, "required_inputs": [], "allows_loop": false, "tool_mode": true}], "field_order": ["index_name", "persist_directory", "ingest_data", "search_query", "allow_dangerous_deserialization", "embedding", "number_of_results"], "beta": false, "legacy": false, "edited": true, "metadata": {}, "tool_mode": false, "lf_version": "1.1.3"}, "showNode": true, "type": "FAISS", "id": "FAISS-SqfVh"}, "selected": false, "measured": {"width": 320, "height": 529}, "dragging": false}, {"id": "ParseData-cAwrl", "type": "genericNode", "position": {"x": 2465.253465694622, "y": 521.9980592359947}, "data": {"node": {"template": {"_type": "Component", "data": {"tool_mode": false, "trace_as_metadata": true, "list": false, "list_add_label": "Add More", "trace_as_input": true, "required": true, "placeholder": "", "show": true, "name": "data", "value": "", "display_name": "Data", "advanced": false, "input_types": ["Data"], "dynamic": false, "info": "The JSON-like dictionary with data nested under 'value' key to convert to a message.", "title_case": false, "type": "other", "_input_type": "DataInput"}, "code": {"type": "code", "required": true, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "from langflow.custom import Component\nfrom langflow.io import DataInput, Output, StrInput\nfrom langflow.schema import Data\nfrom langflow.schema.message import Message\n\nclass ParseDataComponent(Component):\n    display_name = \"Data to Message\"\n    description = \"Convert a nested JSON-like dictionary (under 'value' key) into a single Message with formatted key-value pairs.\"\n    icon = \"message-square\"\n    name = \"ParseData\"\n\n    inputs = [\n        DataInput(\n            name=\"data\",\n            display_name=\"Data\",\n            info=\"The JSON-like dictionary with data nested under 'value' key to convert to a message.\",\n            is_list=False,\n            required=True\n        ),\n        StrInput(\n            name=\"sep\",\n            display_name=\"Separator\",\n            info=\"The separator to use between key-value pairs.\",\n            value=\"\\n\",\n            advanced=True\n        ),\n    ]\n\n    outputs = [\n        Output(\n            display_name=\"Message\",\n            name=\"text\",\n            info=\"Nested dictionary (from 'value') formatted into a single Message, with key-value pairs separated by Separator.\",\n            method=\"parse_data\",\n        ),\n    ]\n\n    def _clean_args(self) -> tuple[Data, str]:\n        data = self.data\n        if data is None:\n            return Data({}), self.sep\n        if isinstance(data, list):\n            data = data[0] if data else {}\n        if isinstance(data, dict):\n            data = data.get(\"value\", data)\n            data = Data(data)\n        elif not isinstance(data, Data):\n            raise ValueError(f\"Input must be a dictionary or Data object, got {type(data).__name__}: {data}\")\n        sep = self.sep\n        return data, sep\n\n    def parse_data(self) -> Message:\n        data, sep = self._clean_args()\n        data_dict = data if isinstance(data, dict) else data.data\n        \n        def format_value(value, indent=0):\n            indent_str = \"  \" * indent\n            if isinstance(value, dict):\n                return sep.join(f\"{indent_str}{k}: {format_value(v, indent + 1)}\" for k, v in value.items())\n            elif isinstance(value, list):\n                return sep.join(f\"{indent_str}- {format_value(item, indent + 1)}\" for item in value)\n            else:\n                return str(value)\n\n        result_string = sep.join(f\"{key}: {format_value(value, 1)}\" for key, value in data_dict.items())\n        if not result_string.strip():\n            result_string = \"No valid data found.\"\n        self.status = result_string\n        return Message(text=result_string)", "fileTypes": [], "file_path": "", "password": false, "name": "code", "advanced": true, "dynamic": true, "info": "", "load_from_db": false, "title_case": false}, "sep": {"tool_mode": false, "trace_as_metadata": true, "load_from_db": false, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "sep", "value": "\n", "display_name": "Separator", "advanced": true, "dynamic": false, "info": "The separator to use between key-value pairs.", "title_case": false, "type": "str", "_input_type": "StrInput"}}, "description": "Convert a nested JSON-like dictionary (under 'value' key) into a single Message with formatted key-value pairs.", "icon": "message-square", "base_classes": ["Message"], "display_name": "Data to Message", "documentation": "", "minimized": false, "custom_fields": {}, "output_types": [], "pinned": false, "conditional_paths": [], "frozen": false, "outputs": [{"types": ["Message"], "selected": "Message", "name": "text", "hidden": null, "display_name": "Message", "method": "parse_data", "value": "__UNDEFINED__", "cache": true, "required_inputs": null, "allows_loop": false}], "field_order": ["data", "sep"], "beta": false, "legacy": false, "edited": true, "metadata": {}, "tool_mode": false, "lf_version": "1.1.3"}, "showNode": true, "type": "ParseData", "id": "ParseData-cAwrl"}, "selected": false, "measured": {"width": 320, "height": 231}, "dragging": false}, {"id": "Prompt-TGd3y", "type": "genericNode", "position": {"x": 3056.055397047626, "y": 892.5380976901571}, "data": {"node": {"template": {"_type": "Component", "code": {"type": "code", "required": true, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import MessageTextInput, Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n        MessageTextInput(\n            name=\"tool_placeholder\",\n            display_name=\"Tool Placeholder\",\n            tool_mode=True,\n            advanced=True,\n            info=\"A placeholder input for tool mode.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    async def update_frontend_node(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = await super().update_frontend_node(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n", "fileTypes": [], "file_path": "", "password": false, "name": "code", "advanced": true, "dynamic": true, "info": "", "load_from_db": false, "title_case": false}, "template": {"tool_mode": false, "trace_as_input": true, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "template", "value": "You are an AI assistant. Given an issue description, analyze the problem and generate a step-by-step solution based on the provided context from FAISS. If the context does not contain enough information, acknowledge this politely and suggest alternative troubleshooting steps or next actions.\n\nOutput Structure:\n\n {{\"actionPlan\": \"step1\": \"Step description\"/\"step2\": \"Step description\"}}\n\nContext:\n{context}\n\nUser Query:\n{query}\n\n-- Return only the above json format and not any other explanation", "display_name": "Template", "advanced": false, "dynamic": false, "info": "", "title_case": false, "type": "prompt", "_input_type": "PromptInput"}, "tool_placeholder": {"tool_mode": true, "trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "tool_placeholder", "value": "", "display_name": "Tool Placeholder", "advanced": true, "input_types": ["Message"], "dynamic": false, "info": "A placeholder input for tool mode.", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "context": {"field_type": "str", "required": false, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "", "fileTypes": [], "file_path": "", "name": "context", "display_name": "context", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "", "load_from_db": false, "title_case": false, "type": "str"}, "query": {"field_type": "str", "required": false, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "", "fileTypes": [], "file_path": "", "name": "query", "display_name": "query", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "", "load_from_db": false, "title_case": false, "type": "str"}}, "description": "Create a prompt template with dynamic variables.", "icon": "prompts", "is_input": null, "is_output": null, "is_composition": null, "base_classes": ["Message"], "name": "", "display_name": "Prompt", "documentation": "", "minimized": false, "custom_fields": {"template": ["context", "query"]}, "output_types": [], "full_path": null, "pinned": false, "conditional_paths": [], "frozen": false, "outputs": [{"types": ["Message"], "selected": "Message", "name": "prompt", "hidden": null, "display_name": "Prompt Message", "method": "build_prompt", "value": "__UNDEFINED__", "cache": true, "required_inputs": null, "allows_loop": false}], "field_order": ["template", "tool_placeholder"], "beta": false, "legacy": false, "error": null, "edited": false, "metadata": {}, "tool_mode": false, "lf_version": "1.1.3"}, "showNode": true, "type": "Prompt", "id": "Prompt-TGd3y"}, "selected": false, "measured": {"width": 320, "height": 419}, "dragging": false}, {"id": "OpenAIModel-botx1", "type": "genericNode", "position": {"x": 3504.786834231042, "y": 866.142130797015}, "data": {"node": {"template": {"_type": "Component", "api_key": {"load_from_db": false, "required": true, "placeholder": "", "show": true, "name": "api_key", "value": null, "display_name": "OpenAI API Key", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "The OpenAI API Key to use for the OpenAI model.", "title_case": false, "password": true, "type": "str", "_input_type": "SecretStrInput"}, "code": {"type": "code", "required": true, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "from langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.base.models.openai_constants import OPENAI_MODEL_NAMES\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import BoolInput, DictInput, DropdownInput, IntInput, SecretStrInput, SliderInput, StrInput\n\n\nclass OpenAIModelComponent(LCModelComponent):\n    display_name = \"OpenAI\"\n    description = \"Generates text using OpenAI LLMs.\"\n    icon = \"OpenAI\"\n    name = \"OpenAIModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n            range_spec=RangeSpec(min=0, max=128000),\n        ),\n        DictInput(\n            name=\"model_kwargs\",\n            display_name=\"Model Kwargs\",\n            advanced=True,\n            info=\"Additional keyword arguments to pass to the model.\",\n        ),\n        BoolInput(\n            name=\"json_mode\",\n            display_name=\"JSON Mode\",\n            advanced=True,\n            info=\"If True, it will output JSON regardless of passing a schema.\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            options=OPENAI_MODEL_NAMES,\n            value=OPENAI_MODEL_NAMES[0],\n        ),\n        StrInput(\n            name=\"openai_api_base\",\n            display_name=\"OpenAI API Base\",\n            advanced=True,\n            info=\"The base URL of the OpenAI API. \"\n            \"Defaults to https://api.openai.com/v1. \"\n            \"You can change this to use other APIs like JinaChat, LocalAI and Prem.\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"OpenAI API Key\",\n            info=\"The OpenAI API Key to use for the OpenAI model.\",\n            advanced=False,\n            value=\"OPENAI_API_KEY\",\n            required=True,\n        ),\n        SliderInput(\n            name=\"temperature\", display_name=\"Temperature\", value=0.1, range_spec=RangeSpec(min=0, max=1, step=0.01)\n        ),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n        IntInput(\n            name=\"max_retries\",\n            display_name=\"Max Retries\",\n            info=\"The maximum number of retries to make when generating.\",\n            advanced=True,\n            value=5,\n        ),\n        IntInput(\n            name=\"timeout\",\n            display_name=\"Timeout\",\n            info=\"The timeout for requests to OpenAI completion API.\",\n            advanced=True,\n            value=700,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        openai_api_key = self.api_key\n        temperature = self.temperature\n        model_name: str = self.model_name\n        max_tokens = self.max_tokens\n        model_kwargs = self.model_kwargs or {}\n        openai_api_base = self.openai_api_base or \"https://api.openai.com/v1\"\n        json_mode = self.json_mode\n        seed = self.seed\n        max_retries = self.max_retries\n        timeout = self.timeout\n\n        api_key = SecretStr(openai_api_key).get_secret_value() if openai_api_key else None\n        output = ChatOpenAI(\n            max_tokens=max_tokens or None,\n            model_kwargs=model_kwargs,\n            model=model_name,\n            base_url=openai_api_base,\n            api_key=api_key,\n            temperature=temperature if temperature is not None else 0.1,\n            seed=seed,\n            max_retries=max_retries,\n            request_timeout=timeout,\n        )\n        if json_mode:\n            output = output.bind(response_format={\"type\": \"json_object\"})\n\n        return output\n\n    def _get_exception_message(self, e: Exception):\n        \"\"\"Get a message from an OpenAI exception.\n\n        Args:\n            e (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n        try:\n            from openai import BadRequestError\n        except ImportError:\n            return None\n        if isinstance(e, BadRequestError):\n            message = e.body.get(\"message\")\n            if message:\n                return message\n        return None\n", "fileTypes": [], "file_path": "", "password": false, "name": "code", "advanced": true, "dynamic": true, "info": "", "load_from_db": false, "title_case": false}, "input_value": {"trace_as_input": true, "tool_mode": false, "trace_as_metadata": true, "load_from_db": false, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "input_value", "value": "", "display_name": "Input", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "", "title_case": false, "type": "str", "_input_type": "MessageInput"}, "json_mode": {"tool_mode": false, "trace_as_metadata": true, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "json_mode", "value": false, "display_name": "JSON Mode", "advanced": true, "dynamic": false, "info": "If True, it will output JSON regardless of passing a schema.", "title_case": false, "type": "bool", "_input_type": "BoolInput"}, "max_retries": {"tool_mode": false, "trace_as_metadata": true, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "max_retries", "value": 5, "display_name": "Max Retries", "advanced": true, "dynamic": false, "info": "The maximum number of retries to make when generating.", "title_case": false, "type": "int", "_input_type": "IntInput"}, "max_tokens": {"tool_mode": false, "trace_as_metadata": true, "range_spec": {"step_type": "float", "min": 0, "max": 128000, "step": 0.1}, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "max_tokens", "value": "", "display_name": "Max Tokens", "advanced": true, "dynamic": false, "info": "The maximum number of tokens to generate. Set to 0 for unlimited tokens.", "title_case": false, "type": "int", "_input_type": "IntInput"}, "model_kwargs": {"tool_mode": false, "trace_as_input": true, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "model_kwargs", "value": {}, "display_name": "Model Kwargs", "advanced": true, "dynamic": false, "info": "Additional keyword arguments to pass to the model.", "title_case": false, "type": "dict", "_input_type": "DictInput"}, "model_name": {"tool_mode": false, "trace_as_metadata": true, "options": ["gpt-4o-mini", "gpt-4o", "gpt-4-turbo", "gpt-4-turbo-preview", "gpt-4", "gpt-3.5-turbo", "gpt-3.5-turbo-0125", "unsloth-llama3.1"], "options_metadata": [], "combobox": false, "dialog_inputs": {}, "required": false, "placeholder": "", "show": true, "name": "model_name", "value": "unsloth-llama3.1", "display_name": "Model Name", "advanced": false, "dynamic": false, "info": "", "title_case": false, "type": "str", "_input_type": "DropdownInput", "load_from_db": false}, "openai_api_base": {"tool_mode": false, "trace_as_metadata": true, "load_from_db": false, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "openai_api_base", "value": "https://dev-vllm-unsloth.bautomate.com/v1", "display_name": "OpenAI API Base", "advanced": true, "dynamic": false, "info": "The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.", "title_case": false, "type": "str", "_input_type": "StrInput"}, "seed": {"tool_mode": false, "trace_as_metadata": true, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "seed", "value": 1, "display_name": "Seed", "advanced": true, "dynamic": false, "info": "The seed controls the reproducibility of the job.", "title_case": false, "type": "int", "_input_type": "IntInput"}, "stream": {"tool_mode": false, "trace_as_metadata": true, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "stream", "value": false, "display_name": "Stream", "advanced": false, "dynamic": false, "info": "Stream the response from the model. Streaming works only in Chat.", "title_case": false, "type": "bool", "_input_type": "BoolInput"}, "system_message": {"tool_mode": false, "trace_as_input": true, "multiline": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "system_message", "value": "", "display_name": "System Message", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "System message to pass to the model.", "title_case": false, "type": "str", "_input_type": "MultilineInput"}, "temperature": {"tool_mode": false, "min_label": "", "max_label": "", "min_label_icon": "", "max_label_icon": "", "slider_buttons": false, "slider_buttons_options": [], "slider_input": false, "range_spec": {"step_type": "float", "min": 0, "max": 1, "step": 0.01}, "required": false, "placeholder": "", "show": true, "name": "temperature", "value": 0.1, "display_name": "Temperature", "advanced": false, "dynamic": false, "info": "", "title_case": false, "type": "slider", "_input_type": "SliderInput"}, "timeout": {"tool_mode": false, "trace_as_metadata": true, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "timeout", "value": 700, "display_name": "Timeout", "advanced": true, "dynamic": false, "info": "The timeout for requests to OpenAI completion API.", "title_case": false, "type": "int", "_input_type": "IntInput"}}, "description": "Generates text using OpenAI LLMs.", "icon": "OpenAI", "base_classes": ["LanguageModel", "Message"], "display_name": "OpenAI", "documentation": "", "minimized": false, "custom_fields": {}, "output_types": [], "pinned": false, "conditional_paths": [], "frozen": false, "outputs": [{"types": ["Message"], "selected": "Message", "name": "text_output", "hidden": null, "display_name": "Message", "method": "text_response", "value": "__UNDEFINED__", "cache": true, "required_inputs": [], "allows_loop": false, "tool_mode": true}, {"types": ["LanguageModel"], "selected": "LanguageModel", "name": "model_output", "hidden": null, "display_name": "Language Model", "method": "build_model", "value": "__UNDEFINED__", "cache": true, "required_inputs": ["api_key"], "allows_loop": false, "tool_mode": true}], "field_order": ["input_value", "system_message", "stream", "max_tokens", "model_kwargs", "json_mode", "model_name", "openai_api_base", "api_key", "temperature", "seed", "max_retries", "timeout"], "beta": false, "legacy": false, "edited": false, "metadata": {}, "tool_mode": false}, "showNode": true, "type": "OpenAIModel", "id": "OpenAIModel-botx1"}, "selected": false, "measured": {"width": 320, "height": 653}}, {"id": "ChatOutput-v8dd1", "type": "genericNode", "position": {"x": 4550.125225579364, "y": 1079.414146421805}, "data": {"node": {"template": {"_type": "Component", "background_color": {"tool_mode": false, "trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "background_color", "value": "", "display_name": "Background Color", "advanced": true, "input_types": ["Message"], "dynamic": false, "info": "The background color of the icon.", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "chat_icon": {"tool_mode": false, "trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "chat_icon", "value": "", "display_name": "Icon", "advanced": true, "input_types": ["Message"], "dynamic": false, "info": "The icon of the message.", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "code": {"type": "code", "required": true, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "from langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.io import DropdownInput, MessageInput, MessageTextInput, Output\nfrom langflow.schema.message import Message\nfrom langflow.schema.properties import Source\nfrom langflow.utils.constants import (\n    MESSAGE_SENDER_AI,\n    MESSAGE_SENDER_NAME_AI,\n    MESSAGE_SENDER_USER,\n)\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    icon = \"MessagesSquare\"\n    name = \"ChatOutput\"\n    minimized = True\n\n    inputs = [\n        MessageInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Message to be passed as output.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_AI,\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_AI,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n        MessageTextInput(\n            name=\"background_color\",\n            display_name=\"Background Color\",\n            info=\"The background color of the icon.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"chat_icon\",\n            display_name=\"Icon\",\n            info=\"The icon of the message.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"text_color\",\n            display_name=\"Text Color\",\n            info=\"The text color of the name\",\n            advanced=True,\n        ),\n    ]\n    outputs = [\n        Output(\n            display_name=\"Message\",\n            name=\"message\",\n            method=\"message_response\",\n        ),\n    ]\n\n    def _build_source(self, id_: str | None, display_name: str | None, source: str | None) -> Source:\n        source_dict = {}\n        if id_:\n            source_dict[\"id\"] = id_\n        if display_name:\n            source_dict[\"display_name\"] = display_name\n        if source:\n            source_dict[\"source\"] = source\n        return Source(**source_dict)\n\n    async def message_response(self) -> Message:\n        source, icon, display_name, source_id = self.get_properties_from_source_component()\n        background_color = self.background_color\n        text_color = self.text_color\n        if self.chat_icon:\n            icon = self.chat_icon\n        message = self.input_value if isinstance(self.input_value, Message) else Message(text=self.input_value)\n        message.sender = self.sender\n        message.sender_name = self.sender_name\n        message.session_id = self.session_id\n        message.flow_id = self.graph.flow_id if hasattr(self, \"graph\") else None\n        message.properties.source = self._build_source(source_id, display_name, source)\n        message.properties.icon = icon\n        message.properties.background_color = background_color\n        message.properties.text_color = text_color\n        if self.session_id and isinstance(message, Message) and self.should_store_message:\n            stored_message = await self.send_message(\n                message,\n            )\n            self.message.value = stored_message\n            message = stored_message\n\n        self.status = message\n        return message\n", "fileTypes": [], "file_path": "", "password": false, "name": "code", "advanced": true, "dynamic": true, "info": "", "load_from_db": false, "title_case": false}, "data_template": {"tool_mode": false, "trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "data_template", "value": "{text}", "display_name": "Data Template", "advanced": true, "input_types": ["Message"], "dynamic": false, "info": "Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "input_value": {"trace_as_input": true, "tool_mode": false, "trace_as_metadata": true, "load_from_db": false, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "input_value", "value": "", "display_name": "Text", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "Message to be passed as output.", "title_case": false, "type": "str", "_input_type": "MessageInput"}, "sender": {"tool_mode": false, "trace_as_metadata": true, "options": ["Machine", "User"], "options_metadata": [], "combobox": false, "dialog_inputs": {}, "required": false, "placeholder": "", "show": true, "name": "sender", "value": "Machine", "display_name": "Sender Type", "advanced": true, "dynamic": false, "info": "Type of sender.", "title_case": false, "type": "str", "_input_type": "DropdownInput"}, "sender_name": {"tool_mode": false, "trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "sender_name", "value": "AI", "display_name": "Sender Name", "advanced": true, "input_types": ["Message"], "dynamic": false, "info": "Name of the sender.", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "session_id": {"tool_mode": false, "trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "session_id", "value": "", "display_name": "Session ID", "advanced": true, "input_types": ["Message"], "dynamic": false, "info": "The session ID of the chat. If empty, the current session ID parameter will be used.", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "should_store_message": {"tool_mode": false, "trace_as_metadata": true, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "should_store_message", "value": true, "display_name": "Store Messages", "advanced": true, "dynamic": false, "info": "Store the message in the history.", "title_case": false, "type": "bool", "_input_type": "BoolInput"}, "text_color": {"tool_mode": false, "trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "text_color", "value": "", "display_name": "Text Color", "advanced": true, "input_types": ["Message"], "dynamic": false, "info": "The text color of the name", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}}, "description": "Display a chat message in the Playground.", "icon": "MessagesSquare", "base_classes": ["Message"], "display_name": "Chat Output", "documentation": "", "minimized": true, "custom_fields": {}, "output_types": [], "pinned": false, "conditional_paths": [], "frozen": false, "outputs": [{"types": ["Message"], "selected": "Message", "name": "message", "display_name": "Message", "method": "message_response", "value": "__UNDEFINED__", "cache": true, "allows_loop": false, "tool_mode": true}], "field_order": ["input_value", "should_store_message", "sender", "sender_name", "session_id", "data_template", "background_color", "chat_icon", "text_color"], "beta": false, "legacy": false, "edited": false, "metadata": {}, "tool_mode": false, "lf_version": "1.1.3"}, "showNode": true, "type": "ChatOutput", "id": "ChatOutput-v8dd1"}, "selected": false, "measured": {"width": 320, "height": 229}, "dragging": false}, {"id": "ConditionalRouter-Wxm5a", "type": "genericNode", "position": {"x": 1241.6347847959164, "y": 255.42029316972753}, "data": {"node": {"template": {"_type": "Component", "case_sensitive": {"tool_mode": false, "trace_as_metadata": true, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "case_sensitive", "value": false, "display_name": "Case Sensitive", "advanced": false, "dynamic": false, "info": "If true, the comparison will be case sensitive.", "title_case": false, "type": "bool", "_input_type": "BoolInput"}, "code": {"type": "code", "required": true, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "import re\nimport json\nfrom langflow.custom import Component\nfrom langflow.io import BoolInput, DropdownInput, IntInput, MessageInput, MessageTextInput, Output\nfrom langflow.schema.message import Message\n\n\nclass ConditionalRouterComponent(Component):\n    display_name = \"If-Else\"\n    description = \"Routes an input message to a corresponding output based on text comparison.\"\n    icon = \"split\"\n    name = \"ConditionalRouter\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.__iteration_updated = False\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_text\",\n            display_name=\"Text Input\",\n            info=\"The primary text input for the operation.\",\n            required=True,\n        ),\n        MessageTextInput(\n            name=\"match_text\",\n            display_name=\"Match Text\",\n            info=\"The text input to compare against.\",\n            required=True,\n        ),\n        DropdownInput(\n            name=\"operator\",\n            display_name=\"Operator\",\n            options=[\"equals\", \"not equals\", \"contains\", \"starts with\", \"ends with\", \"regex\"],\n            info=\"The operator to apply for comparing the texts.\",\n            value=\"equals\",\n            real_time_refresh=True,\n        ),\n        BoolInput(\n            name=\"case_sensitive\",\n            display_name=\"Case Sensitive\",\n            info=\"If true, the comparison will be case sensitive.\",\n            value=False,\n        ),\n        MessageInput(\n            name=\"message\",\n            display_name=\"Message\",\n            info=\"The message to pass through either route.\",\n        ),\n        IntInput(\n            name=\"max_iterations\",\n            display_name=\"Max Iterations\",\n            info=\"The maximum number of iterations for the conditional router.\",\n            value=10,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"default_route\",\n            display_name=\"Default Route\",\n            options=[\"true_result\", \"false_result\"],\n            info=\"The default route to take when max iterations are reached.\",\n            value=\"false_result\",\n            advanced=True,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"True\", name=\"true_result\", method=\"true_response\"),\n        Output(display_name=\"False\", name=\"false_result\", method=\"false_response\"),\n    ]\n\n    def _pre_run_setup(self):\n        self.__iteration_updated = False\n\n    def evaluate_condition(self, input_text: str, match_text: str, operator: str, *, case_sensitive: bool) -> bool:\n        if not case_sensitive and operator != \"regex\":\n            input_text = input_text.lower()\n            match_text = match_text.lower()\n\n        if operator == \"equals\":\n            return input_text == match_text\n        if operator == \"not equals\":\n            return input_text != match_text\n        if operator == \"contains\":\n            return match_text in input_text\n        if operator == \"starts with\":\n            return input_text.startswith(match_text)\n        if operator == \"ends with\":\n            return input_text.endswith(match_text)\n        if operator == \"regex\":\n            try:\n                return bool(re.match(match_text, input_text))\n            except re.error:\n                return False\n        return False\n\n    def iterate_and_stop_once(self, route_to_stop: str):\n        if not self.__iteration_updated:\n            self.update_ctx({f\"{self._id}_iteration\": self.ctx.get(f\"{self._id}_iteration\", 0) + 1})\n            self.__iteration_updated = True\n            if self.ctx.get(f\"{self._id}_iteration\", 0) >= self.max_iterations and route_to_stop == self.default_route:\n                route_to_stop = \"true_result\" if route_to_stop == \"false_result\" else \"false_result\"\n            self.stop(route_to_stop)\n\n    def true_response(self) -> Message:\n        print(f\"Input type: {type(self.input_text)}, Input value: {self.input_text}\")\n        print(f\"Match text type: {type(self.match_text)}, Match text value: {self.match_text}\")\n\n        # Handle match_text if it's a Message object\n        match_text_value = self.match_text\n        if isinstance(self.match_text, Message):\n            match_text_value = getattr(self.match_text, \"text\", getattr(self.match_text, \"content\", str(self.match_text)))\n        elif not isinstance(self.match_text, str):\n            match_text_value = str(self.match_text)\n        print(f\"Match text after extraction: '{match_text_value}'\")\n\n        # Handle input_text: if it's a string, try to parse it as JSON\n        input_data = self.input_text\n        if isinstance(self.input_text, str):\n            try:\n                input_data = json.loads(self.input_text)\n                print(f\"Parsed input as list: {input_data}\")\n            except json.JSONDecodeError:\n                print(\"Input is a string but not valid JSON, treating as string\")\n        \n        # Now process input_data as a list\n        if isinstance(input_data, list):\n            classification = None\n            description = None\n            # Extract classification and description from the list\n            for item in input_data:\n                if isinstance(item, dict):\n                    if \"classification\" in item:\n                        classification = item.get(\"classification\", \"\")\n                    if \"description\" in item:\n                        description = item.get(\"description\", \"\")\n            \n            print(f\"Extracted classification: '{classification}', Description: '{description}'\")\n            \n            # Proceed only if both values are found\n            if classification is not None and description is not None:\n                # Compare classification with match_text\n                classification_str = str(classification).strip()\n                match_text_str = str(match_text_value).strip()\n                print(f\"Comparing: '{classification_str}' == '{match_text_str}'\")\n                # Additional debug: compare lengths and character codes\n                print(f\"Length of classification: {len(classification_str)}, Length of match_text: {len(match_text_str)}\")\n                print(f\"Classification chars: {[ord(c) for c in classification_str]}\")\n                print(f\"Match text chars: {[ord(c) for c in match_text_str]}\")\n                if classification_str == match_text_str:\n                    print(f\"Match found! Returning description: '{description}'\")\n                    self.status = description\n                    self.iterate_and_stop_once(\"false_result\")\n                    message = Message(text=description, content=description)\n                    print(f\"Returning Message: text='{message.text}', content='{getattr(message, 'content', None)}'\")\n                    return message\n                else:\n                    print(f\"No match: '{classification_str}' != '{match_text_str}'\")\n                    self.iterate_and_stop_once(\"true_result\")\n                    message = Message(text=\"Condition was False\", content=\"Condition was False\")\n                    print(f\"Returning Message: text='{message.text}', content='{getattr(message, 'content', None)}'\")\n                    return message\n            else:\n                print(\"Classification or description not found in list\")\n                self.iterate_and_stop_once(\"true_result\")\n                message = Message(text=\"Condition was False\", content=\"Condition was False\")\n                print(f\"Returning Message: text='{message.text}', content='{getattr(message, 'content', None)}'\")\n                return message\n        \n        # Fallback for non-list input\n        input_text = input_data if isinstance(input_data, str) else input_data.get(\"subject\", \"\")\n        print(f\"Fallback - Input text: '{input_text}'\")\n        result = self.evaluate_condition(\n            input_text, match_text_value, self.operator, case_sensitive=self.case_sensitive\n        )\n        print(f\"Fallback logic - Result: {result}\")\n        if result:\n            self.status = input_text\n            self.iterate_and_stop_once(\"false_result\")\n            message = Message(text=input_text, content=input_text)\n            print(f\"Returning Message: text='{message.text}', content='{getattr(message, 'content', None)}'\")\n            return message\n        self.iterate_and_stop_once(\"true_result\")\n        message = Message(text=\"Condition was False\", content=\"Condition was False\")\n        print(f\"Returning Message: text='{message.text}', content='{getattr(message, 'content', None)}'\")\n        return message\n\n    def false_response(self) -> Message:\n        input_text = self.input_text if isinstance(self.input_text, str) else self.input_text.get(\"subject\", \"\")\n        result = self.evaluate_condition(\n            input_text, self.match_text, self.operator, case_sensitive=self.case_sensitive\n        )\n        if not result:\n            self.status = input_text\n            self.iterate_and_stop_once(\"true_result\")\n            return Message(text=input_text, content=input_text)\n        self.iterate_and_stop_once(\"false_result\")\n        return Message(text=\"Condition was True\", content=\"Condition was True\")\n\n    def update_build_config(self, build_config: dict, field_value: str, field_name: str | None = None) -> dict:\n        if field_name == \"operator\":\n            if field_value == \"regex\":\n                build_config.pop(\"case_sensitive\", None)\n            elif \"case_sensitive\" not in build_config:\n                case_sensitive_input = next(\n                    (input_field for input_field in self.inputs if input_field.name == \"case_sensitive\"), None\n                )\n                if case_sensitive_input:\n                    build_config[\"case_sensitive\"] = case_sensitive_input.to_dict()\n        return build_config", "fileTypes": [], "file_path": "", "password": false, "name": "code", "advanced": true, "dynamic": true, "info": "", "load_from_db": false, "title_case": false}, "default_route": {"tool_mode": false, "trace_as_metadata": true, "options": ["true_result", "false_result"], "options_metadata": [], "combobox": false, "dialog_inputs": {}, "required": false, "placeholder": "", "show": true, "name": "default_route", "value": "false_result", "display_name": "Default Route", "advanced": true, "dynamic": false, "info": "The default route to take when max iterations are reached.", "title_case": false, "type": "str", "_input_type": "DropdownInput"}, "input_text": {"tool_mode": false, "trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "list_add_label": "Add More", "required": true, "placeholder": "", "show": true, "name": "input_text", "value": "", "display_name": "Text Input", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "The primary text input for the operation.", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "match_text": {"tool_mode": false, "trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "list_add_label": "Add More", "required": true, "placeholder": "", "show": true, "name": "match_text", "value": "L1 (Low)", "display_name": "Match Text", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "The text input to compare against.", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "max_iterations": {"tool_mode": false, "trace_as_metadata": true, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "max_iterations", "value": 10, "display_name": "Max Iterations", "advanced": true, "dynamic": false, "info": "The maximum number of iterations for the conditional router.", "title_case": false, "type": "int", "_input_type": "IntInput"}, "message": {"trace_as_input": true, "tool_mode": false, "trace_as_metadata": true, "load_from_db": false, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "message", "value": "", "display_name": "Message", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "The message to pass through either route.", "title_case": false, "type": "str", "_input_type": "MessageInput"}, "operator": {"tool_mode": false, "trace_as_metadata": true, "options": ["equals", "not equals", "contains", "starts with", "ends with", "regex"], "options_metadata": [], "combobox": false, "dialog_inputs": {}, "required": false, "placeholder": "", "show": true, "name": "operator", "value": "equals", "display_name": "Operator", "advanced": false, "dynamic": false, "info": "The operator to apply for comparing the texts.", "real_time_refresh": true, "title_case": false, "type": "str", "_input_type": "DropdownInput"}}, "description": "Routes an input message to a corresponding output based on text comparison.", "icon": "split", "base_classes": ["Message"], "display_name": "If-Else", "documentation": "", "minimized": false, "custom_fields": {}, "output_types": [], "pinned": false, "conditional_paths": [], "frozen": false, "outputs": [{"types": ["Message"], "selected": "Message", "name": "true_result", "hidden": null, "display_name": "True", "method": "true_response", "value": "__UNDEFINED__", "cache": true, "required_inputs": null, "allows_loop": false}, {"types": ["Message"], "selected": "Message", "name": "false_result", "hidden": null, "display_name": "False", "method": "false_response", "value": "__UNDEFINED__", "cache": true, "required_inputs": null, "allows_loop": false}], "field_order": ["input_text", "match_text", "operator", "case_sensitive", "message", "max_iterations", "default_route"], "beta": false, "legacy": false, "edited": true, "metadata": {}, "tool_mode": false, "lf_version": "1.1.3"}, "showNode": true, "type": "ConditionalRouter", "id": "ConditionalRouter-Wxm5a"}, "selected": false, "measured": {"width": 320, "height": 585}, "dragging": false}, {"id": "TextOutput-5j9L7", "type": "genericNode", "position": {"x": 1610.8862754148627, "y": 281.9252039107467}, "data": {"node": {"template": {"_type": "Component", "code": {"type": "code", "required": true, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "from langflow.base.io.text import TextComponent\nfrom langflow.io import MultilineInput, Output\nfrom langflow.schema.message import Message\n\n\nclass TextOutputComponent(TextComponent):\n    display_name = \"Text Output\"\n    description = \"Display a text output in the Playground.\"\n    icon = \"type\"\n    name = \"TextOutput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as output.\",\n            value=\"\",  # Default value\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        # Handle Message object input and extract text/content\n        if isinstance(self.input_value, Message):\n            text_content = getattr(self.input_value, \"text\", self.input_value.content)\n        else:\n            text_content = str(self.input_value)  # Fallback for unexpected types\n        message = Message(text=text_content)\n        self.status = text_content\n        print(f\"TextOutput received: {text_content}\")  # Debug output\n        return message", "fileTypes": [], "file_path": "", "password": false, "name": "code", "advanced": true, "dynamic": true, "info": "", "load_from_db": false, "title_case": false}, "input_value": {"tool_mode": false, "trace_as_input": true, "multiline": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "input_value", "value": "", "display_name": "Text", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "Text to be passed as output.", "title_case": false, "type": "str", "_input_type": "MultilineInput"}}, "description": "Display a text output in the Playground.", "icon": "type", "base_classes": ["Message"], "display_name": "Text Output", "documentation": "", "minimized": false, "custom_fields": {}, "output_types": [], "pinned": false, "conditional_paths": [], "frozen": false, "outputs": [{"types": ["Message"], "selected": "Message", "name": "text", "hidden": null, "display_name": "Message", "method": "text_response", "value": "__UNDEFINED__", "cache": true, "required_inputs": null, "allows_loop": false}], "field_order": ["input_value"], "beta": false, "legacy": false, "edited": true, "metadata": {}, "tool_mode": false, "lf_version": "1.1.3"}, "showNode": true, "type": "TextOutput", "id": "TextOutput-5j9L7"}, "selected": false, "measured": {"width": 320, "height": 229}, "dragging": false}, {"id": "JSONCleaner-rzxHY", "type": "genericNode", "position": {"x": 1980.836859411052, "y": -5.919000474044168}, "data": {"node": {"template": {"_type": "Component", "code": {"type": "code", "required": true, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "import json\nimport unicodedata\n\nfrom langflow.custom import Component\nfrom langflow.inputs import BoolInput, MessageTextInput\nfrom langflow.schema.message import Message\nfrom langflow.template import Output\n\n\nclass JSONCleaner(Component):\n    icon = \"braces\"\n    display_name = \"JSON Cleaner\"\n    description = (\n        \"Cleans the messy and sometimes incorrect JSON strings produced by LLMs \"\n        \"so that they are fully compliant with the JSON spec.\"\n    )\n\n    inputs = [\n        MessageTextInput(\n            name=\"json_str\", display_name=\"JSON String\", info=\"The JSON string to be cleaned.\", required=True\n        ),\n        BoolInput(\n            name=\"remove_control_chars\",\n            display_name=\"Remove Control Characters\",\n            info=\"Remove control characters from the JSON string.\",\n            required=False,\n        ),\n        BoolInput(\n            name=\"normalize_unicode\",\n            display_name=\"Normalize Unicode\",\n            info=\"Normalize Unicode characters in the JSON string.\",\n            required=False,\n        ),\n        BoolInput(\n            name=\"validate_json\",\n            display_name=\"Validate JSON\",\n            info=\"Validate the JSON string to ensure it is well-formed.\",\n            required=False,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Cleaned JSON String\", name=\"output\", method=\"clean_json\"),\n    ]\n\n    def clean_json(self) -> Message:\n        try:\n            from json_repair import repair_json\n        except ImportError as e:\n            msg = \"Could not import the json_repair package. Please install it with `pip install json_repair`.\"\n            raise ImportError(msg) from e\n\n        \"\"\"Clean the input JSON string based on provided options and return the cleaned JSON string.\"\"\"\n        json_str = self.json_str\n        remove_control_chars = self.remove_control_chars\n        normalize_unicode = self.normalize_unicode\n        validate_json = self.validate_json\n\n        start = json_str.find(\"{\")\n        end = json_str.rfind(\"}\")\n        if start == -1 or end == -1:\n            msg = \"Invalid JSON string: Missing '{' or '}'\"\n            raise ValueError(msg)\n        try:\n            json_str = json_str[start : end + 1]\n\n            if remove_control_chars:\n                json_str = self._remove_control_characters(json_str)\n            if normalize_unicode:\n                json_str = self._normalize_unicode(json_str)\n            if validate_json:\n                json_str = self._validate_json(json_str)\n\n            cleaned_json_str = repair_json(json_str)\n            result = str(cleaned_json_str)\n\n            self.status = result\n            return Message(text=result)\n        except Exception as e:\n            msg = f\"Error cleaning JSON string: {e}\"\n            raise ValueError(msg) from e\n\n    def _remove_control_characters(self, s: str) -> str:\n        \"\"\"Remove control characters from the string.\"\"\"\n        return s.translate(self.translation_table)\n\n    def _normalize_unicode(self, s: str) -> str:\n        \"\"\"Normalize Unicode characters in the string.\"\"\"\n        return unicodedata.normalize(\"NFC\", s)\n\n    def _validate_json(self, s: str) -> str:\n        \"\"\"Validate the JSON string.\"\"\"\n        try:\n            json.loads(s)\n        except json.JSONDecodeError as e:\n            msg = f\"Invalid JSON string: {e}\"\n            raise ValueError(msg) from e\n        return s\n\n    def __init__(self, *args, **kwargs):\n        # Create a translation table that maps control characters to None\n        super().__init__(*args, **kwargs)\n        self.translation_table = str.maketrans(\"\", \"\", \"\".join(chr(i) for i in range(32)) + chr(127))\n", "fileTypes": [], "file_path": "", "password": false, "name": "code", "advanced": true, "dynamic": true, "info": "", "load_from_db": false, "title_case": false}, "json_str": {"tool_mode": false, "trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "list_add_label": "Add More", "required": true, "placeholder": "", "show": true, "name": "json_str", "value": "", "display_name": "JSON String", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "The JSON string to be cleaned.", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "normalize_unicode": {"tool_mode": false, "trace_as_metadata": true, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "normalize_unicode", "value": false, "display_name": "Normalize Unicode", "advanced": false, "dynamic": false, "info": "Normalize Unicode characters in the JSON string.", "title_case": false, "type": "bool", "_input_type": "BoolInput"}, "remove_control_chars": {"tool_mode": false, "trace_as_metadata": true, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "remove_control_chars", "value": false, "display_name": "Remove Control Characters", "advanced": false, "dynamic": false, "info": "Remove control characters from the JSON string.", "title_case": false, "type": "bool", "_input_type": "BoolInput"}, "validate_json": {"tool_mode": false, "trace_as_metadata": true, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "validate_json", "value": false, "display_name": "Validate JSON", "advanced": false, "dynamic": false, "info": "Validate the JSON string to ensure it is well-formed.", "title_case": false, "type": "bool", "_input_type": "BoolInput"}}, "description": "Cleans the messy and sometimes incorrect JSON strings produced by LLMs so that they are fully compliant with the JSON spec.", "icon": "braces", "base_classes": ["Message"], "display_name": "JSON Cleaner", "documentation": "", "minimized": false, "custom_fields": {}, "output_types": [], "pinned": false, "conditional_paths": [], "frozen": false, "outputs": [{"types": ["Message"], "selected": "Message", "name": "output", "display_name": "Cleaned JSON String", "method": "clean_json", "value": "__UNDEFINED__", "cache": true, "allows_loop": false}], "field_order": ["json_str", "remove_control_chars", "normalize_unicode", "validate_json"], "beta": false, "legacy": false, "edited": false, "metadata": {}, "tool_mode": false, "category": "processing", "key": "JSONCleaner", "score": 0.007568328950209746, "lf_version": "1.1.3"}, "showNode": true, "type": "JSONCleaner", "id": "JSONCleaner-rzxHY"}, "selected": false, "measured": {"width": 320, "height": 395}, "dragging": false}, {"id": "TextOutput-1jQWG", "type": "genericNode", "position": {"x": 1674.1176252305381, "y": 635.8320125805716}, "data": {"node": {"template": {"_type": "Component", "code": {"type": "code", "required": true, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "from langflow.base.io.text import TextComponent\nfrom langflow.io import MultilineInput, Output\nfrom langflow.schema.message import Message\n\n\nclass TextOutputComponent(TextComponent):\n    display_name = \"Text Output\"\n    description = \"Display a text output in the Playground.\"\n    icon = \"type\"\n    name = \"TextOutput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as output.\",\n            value=\"\",  # Default value\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        # Handle Message object input and extract text/content\n        if isinstance(self.input_value, Message):\n            text_content = getattr(self.input_value, \"text\", self.input_value.content)\n        else:\n            text_content = str(self.input_value)  # Fallback for unexpected types\n        message = Message(text=text_content)\n        self.status = text_content\n        print(f\"TextOutput received: {text_content}\")  # Debug output\n        return message", "fileTypes": [], "file_path": "", "password": false, "name": "code", "advanced": true, "dynamic": true, "info": "", "load_from_db": false, "title_case": false}, "input_value": {"tool_mode": false, "trace_as_input": true, "multiline": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "input_value", "value": "", "display_name": "Text", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "Text to be passed as output.", "title_case": false, "type": "str", "_input_type": "MultilineInput"}}, "description": "Display a text output in the Playground.", "icon": "type", "base_classes": ["Message"], "display_name": "Text Output", "documentation": "", "minimized": false, "custom_fields": {}, "output_types": [], "pinned": false, "conditional_paths": [], "frozen": false, "outputs": [{"types": ["Message"], "selected": "Message", "name": "text", "hidden": null, "display_name": "Message", "method": "text_response", "value": "__UNDEFINED__", "cache": true, "required_inputs": null, "allows_loop": false}], "field_order": ["input_value"], "beta": false, "legacy": false, "edited": true, "metadata": {}, "tool_mode": false, "lf_version": "1.1.3"}, "showNode": true, "type": "TextOutput", "id": "TextOutput-1jQWG"}, "selected": false, "measured": {"width": 320, "height": 229}, "dragging": false}, {"id": "ReplaceText-nHslZ", "type": "genericNode", "position": {"x": 3967.613865592154, "y": 1246.6812478251193}, "data": {"node": {"template": {"_type": "Component", "code": {"type": "code", "required": true, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "import json\nfrom langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema.message import Message\n\n\nclass ReplaceTextComponent(Component):\n    display_name = \"Replace Text\"\n    description = \"Replace a specific key's value in the first JSON with a value from the second JSON.\"\n    icon = \"replace\"\n    name = \"ReplaceText\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"text1\",\n            display_name=\"First Text\",\n            info=\"The JSON text in which to perform the replacement (e.g., a list of dictionaries).\",\n        ),\n        MessageTextInput(\n            name=\"text2\",\n            display_name=\"Second Text\",\n            info=\"The JSON text containing the replacement data (e.g., a dictionary with the new value).\",\n        ),\n        MessageTextInput(\n            name=\"key_to_replace\",\n            display_name=\"Key to Replace\",\n            info=\"The key in the JSON to replace (e.g., 'actionPlan').\",\n            value=\"actionPlan\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Modified Text\", name=\"modified_text\", method=\"replace_text\"),\n    ]\n\n    def replace_text(self) -> Message:\n        # Ensure inputs are strings\n        text1 = str(self.text1) if self.text1 else \"\"\n        text2 = str(self.text2) if self.text2 else \"\"\n        key_to_replace = str(self.key_to_replace) if self.key_to_replace else \"actionPlan\"\n\n        try:\n            # Parse text1 as a JSON list of dictionaries\n            text1_list = json.loads(text1) if isinstance(text1, str) else text1\n            if not isinstance(text1_list, list):\n                raise ValueError(\"text1 must be a JSON list of dictionaries\")\n            print(f\"Parsed text1: {text1_list}\")\n\n            # Parse text2 as a JSON dictionary\n            text2_dict = json.loads(text2) if isinstance(text2, str) else text2\n            if not isinstance(text2_dict, dict):\n                raise ValueError(\"text2 must be a JSON dictionary\")\n            print(f\"Parsed text2: {text2_dict}\")\n\n            # Extract the replacement value from text2\n            if key_to_replace not in text2_dict:\n                raise ValueError(f\"Key '{key_to_replace}' not found in text2\")\n            replacement_value = text2_dict[key_to_replace]\n            print(f\"Replacement value for '{key_to_replace}': {replacement_value}\")\n\n            # Find and replace the key_to_replace in text1_list\n            found = False\n            for item in text1_list:\n                if isinstance(item, dict) and key_to_replace in item:\n                    item[key_to_replace] = replacement_value\n                    found = True\n                    break\n\n            if not found:\n                raise ValueError(f\"Key '{key_to_replace}' not found in text1\")\n\n            # Convert the modified list back to a JSON string\n            modified_text = json.dumps(text1_list, indent=2)\n            print(f\"Modified text: {modified_text}\")\n\n        except json.JSONDecodeError as e:\n            print(f\"JSON parsing error: {e}\")\n            modified_text = text1\n        except Exception as e:\n            print(f\"Error during replacement: {e}\")\n            modified_text = text1\n\n        self.status = modified_text\n        print(f\"Final result: '{modified_text}'\")\n        return Message(text=modified_text, content=modified_text)", "fileTypes": [], "file_path": "", "password": false, "name": "code", "advanced": true, "dynamic": true, "info": "", "load_from_db": false, "title_case": false}, "key_to_replace": {"tool_mode": false, "trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "key_to_replace", "value": "actionPlan", "display_name": "Key to Replace", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "The key in the JSON to replace (e.g., 'actionPlan').", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "text1": {"tool_mode": false, "trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "text1", "value": "", "display_name": "First Text", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "The JSON text in which to perform the replacement (e.g., a list of dictionaries).", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "text2": {"tool_mode": false, "trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "text2", "value": "", "display_name": "Second Text", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "The JSON text containing the replacement data (e.g., a dictionary with the new value).", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}}, "description": "Replace a specific key's value in the first JSON with a value from the second JSON.", "icon": "replace", "base_classes": ["Message"], "display_name": "Replace Text", "documentation": "", "minimized": false, "custom_fields": {}, "output_types": [], "pinned": false, "conditional_paths": [], "frozen": false, "outputs": [{"types": ["Message"], "selected": "Message", "name": "modified_text", "hidden": null, "display_name": "Modified Text", "method": "replace_text", "value": "__UNDEFINED__", "cache": true, "required_inputs": null, "allows_loop": false}], "field_order": ["text1", "text2", "key_to_replace"], "beta": false, "legacy": false, "edited": true, "metadata": {}, "tool_mode": false, "lf_version": "1.1.3"}, "showNode": true, "type": "ReplaceText", "id": "ReplaceText-nHslZ"}, "selected": false, "measured": {"width": 320, "height": 413}, "dragging": false}, {"id": "JSONCleaner-Z9K0c", "type": "genericNode", "position": {"x": 4330.485354190752, "y": 1297.135307435693}, "data": {"node": {"template": {"_type": "Component", "code": {"type": "code", "required": true, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "import json\n\nfrom langflow.custom import Component\nfrom langflow.inputs import BoolInput, MessageTextInput\nfrom langflow.schema.message import Message\nfrom langflow.template import Output\n\n\nclass JSONCleaner(Component):\n    icon = \"braces\"\n    display_name = \"JSON Cleaner\"\n    description = \"Cleans the messy and sometimes incorrect JSON strings produced by LLMs so that they are fully compliant with the JSON spec.\"\n\n    inputs = [\n        MessageTextInput(name=\"json_str\", display_name=\"JSON String\", info=\"The JSON string to be cleaned.\", required=True),\n        BoolInput(name=\"remove_control_chars\", display_name=\"Remove Control Characters\", info=\"Remove control characters from the JSON string.\", required=False),\n        BoolInput(name=\"validate_json\", display_name=\"Validate JSON\", info=\"Validate the JSON string to ensure it is well-formed.\", required=False),\n    ]\n\n    outputs = [\n        Output(display_name=\"Cleaned JSON String\", name=\"output\", method=\"clean_json\"),\n    ]\n\n    def clean_json(self) -> Message:\n        try:\n            from json_repair import repair_json\n        except ImportError as e:\n            raise ImportError(\"Could not import the json_repair package. Please install it with `pip install json_repair`.\") from e\n\n        json_str = self.json_str.strip()\n        remove_control_chars = self.remove_control_chars\n        validate_json = self.validate_json\n\n        json_str = self._extract_json(json_str)\n        if not json_str:\n            raise ValueError(\"Invalid JSON string: No valid JSON object or array found.\")\n\n        try:\n            if remove_control_chars:\n                json_str = self._remove_control_characters(json_str)\n            if validate_json:\n                json_str = self._validate_json(json_str)\n\n            cleaned_json_str = repair_json(json_str)\n            result = str(cleaned_json_str)\n\n            self.status = result\n            return Message(text=result)\n        except Exception as e:\n            raise ValueError(f\"Error cleaning JSON string: {e}\") from e\n\n    def _extract_json(self, s: str) -> str:\n        s = s.strip()\n        stack = []\n        start = -1\n        end = -1\n\n        for i, char in enumerate(s):\n            if char in \"{[\":\n                if start == -1:\n                    start = i\n                stack.append(char)\n            elif char in \"}]\":\n                if not stack:\n                    continue\n                if (char == \"}\" and stack[-1] == \"{\") or (char == \"]\" and stack[-1] == \"[\"):\n                    stack.pop()\n                if not stack:\n                    end = i + 1\n                    break\n\n        if start != -1 and end != -1 and start < end:\n            return s[start:end]\n        return \"\"\n\n    def _remove_control_characters(self, s: str) -> str:\n        return s.translate(self.translation_table)\n\n    def _validate_json(self, s: str) -> str:\n        try:\n            json.loads(s)\n        except json.JSONDecodeError as e:\n            raise ValueError(f\"Invalid JSON string: {e}\") from e\n        return s\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.translation_table = str.maketrans(\"\", \"\", \"\".join(chr(i) for i in range(32)) + chr(127))", "fileTypes": [], "file_path": "", "password": false, "name": "code", "advanced": true, "dynamic": true, "info": "", "load_from_db": false, "title_case": false}, "json_str": {"tool_mode": false, "trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "list_add_label": "Add More", "required": true, "placeholder": "", "show": true, "name": "json_str", "value": "", "display_name": "JSON String", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "The JSON string to be cleaned.", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "remove_control_chars": {"tool_mode": false, "trace_as_metadata": true, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "remove_control_chars", "value": true, "display_name": "Remove Control Characters", "advanced": false, "dynamic": false, "info": "Remove control characters from the JSON string.", "title_case": false, "type": "bool", "_input_type": "BoolInput", "load_from_db": false}, "validate_json": {"tool_mode": false, "trace_as_metadata": true, "list": false, "list_add_label": "Add More", "required": false, "placeholder": "", "show": true, "name": "validate_json", "value": true, "display_name": "Validate JSON", "advanced": false, "dynamic": false, "info": "Validate the JSON string to ensure it is well-formed.", "title_case": false, "type": "bool", "_input_type": "BoolInput", "load_from_db": false}}, "description": "Cleans the messy and sometimes incorrect JSON strings produced by LLMs so that they are fully compliant with the JSON spec.", "icon": "braces", "base_classes": ["Message"], "display_name": "JSON Cleaner", "documentation": "", "minimized": false, "custom_fields": {}, "output_types": [], "pinned": false, "conditional_paths": [], "frozen": false, "outputs": [{"types": ["Message"], "selected": "Message", "name": "output", "hidden": null, "display_name": "Cleaned JSON String", "method": "clean_json", "value": "__UNDEFINED__", "cache": true, "required_inputs": null, "allows_loop": false}], "field_order": ["json_str", "remove_control_chars", "validate_json"], "beta": false, "legacy": false, "edited": true, "metadata": {}, "tool_mode": false, "lf_version": "1.1.3"}, "showNode": true, "type": "JSONCleaner", "id": "JSONCleaner-Z9K0c"}, "selected": false, "measured": {"width": 320, "height": 353}, "dragging": false}], "edges": [{"source": "ChatInput-meIRY", "sourceHandle": "{\u0153dataType\u0153:\u0153ChatInput\u0153,\u0153id\u0153:\u0153ChatInput-meIRY\u0153,\u0153name\u0153:\u0153message\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}", "target": "Prompt-MQYzX", "targetHandle": "{\u0153fieldName\u0153:\u0153content\u0153,\u0153id\u0153:\u0153Prompt-MQYzX\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "data": {"targetHandle": {"fieldName": "content", "id": "Prompt-MQYzX", "inputTypes": ["Message"], "type": "str"}, "sourceHandle": {"dataType": "ChatInput", "id": "ChatInput-meIRY", "name": "message", "output_types": ["Message"]}}, "id": "reactflow__edge-ChatInput-meIRY{\u0153dataType\u0153:\u0153ChatInput\u0153,\u0153id\u0153:\u0153ChatInput-meIRY\u0153,\u0153name\u0153:\u0153message\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}-Prompt-MQYzX{\u0153fieldName\u0153:\u0153content\u0153,\u0153id\u0153:\u0153Prompt-MQYzX\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "animated": false, "className": "", "selected": false}, {"source": "Prompt-MQYzX", "sourceHandle": "{\u0153dataType\u0153:\u0153Prompt\u0153,\u0153id\u0153:\u0153Prompt-MQYzX\u0153,\u0153name\u0153:\u0153prompt\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}", "target": "OpenAIModel-CriS5", "targetHandle": "{\u0153fieldName\u0153:\u0153input_value\u0153,\u0153id\u0153:\u0153OpenAIModel-CriS5\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "data": {"targetHandle": {"fieldName": "input_value", "id": "OpenAIModel-CriS5", "inputTypes": ["Message"], "type": "str"}, "sourceHandle": {"dataType": "Prompt", "id": "Prompt-MQYzX", "name": "prompt", "output_types": ["Message"]}}, "id": "reactflow__edge-Prompt-MQYzX{\u0153dataType\u0153:\u0153Prompt\u0153,\u0153id\u0153:\u0153Prompt-MQYzX\u0153,\u0153name\u0153:\u0153prompt\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}-OpenAIModel-CriS5{\u0153fieldName\u0153:\u0153input_value\u0153,\u0153id\u0153:\u0153OpenAIModel-CriS5\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "animated": false, "className": "", "selected": false}, {"source": "OpenAIModel-CriS5", "sourceHandle": "{\u0153dataType\u0153:\u0153OpenAIModel\u0153,\u0153id\u0153:\u0153OpenAIModel-CriS5\u0153,\u0153name\u0153:\u0153text_output\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}", "target": "JSONCleaner-1mD4u", "targetHandle": "{\u0153fieldName\u0153:\u0153json_str\u0153,\u0153id\u0153:\u0153JSONCleaner-1mD4u\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "data": {"targetHandle": {"fieldName": "json_str", "id": "JSONCleaner-1mD4u", "inputTypes": ["Message"], "type": "str"}, "sourceHandle": {"dataType": "OpenAIModel", "id": "OpenAIModel-CriS5", "name": "text_output", "output_types": ["Message"]}}, "id": "reactflow__edge-OpenAIModel-CriS5{\u0153dataType\u0153:\u0153OpenAIModel\u0153,\u0153id\u0153:\u0153OpenAIModel-CriS5\u0153,\u0153name\u0153:\u0153text_output\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}-JSONCleaner-1mD4u{\u0153fieldName\u0153:\u0153json_str\u0153,\u0153id\u0153:\u0153JSONCleaner-1mD4u\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "animated": false, "className": "", "selected": false}, {"source": "File-DO5zV", "sourceHandle": "{\u0153dataType\u0153:\u0153File\u0153,\u0153id\u0153:\u0153File-DO5zV\u0153,\u0153name\u0153:\u0153data\u0153,\u0153output_types\u0153:[\u0153Data\u0153]}", "target": "SplitText-RnCyU", "targetHandle": "{\u0153fieldName\u0153:\u0153data_inputs\u0153,\u0153id\u0153:\u0153SplitText-RnCyU\u0153,\u0153inputTypes\u0153:[\u0153Data\u0153],\u0153type\u0153:\u0153other\u0153}", "data": {"targetHandle": {"fieldName": "data_inputs", "id": "SplitText-RnCyU", "inputTypes": ["Data"], "type": "other"}, "sourceHandle": {"dataType": "File", "id": "File-DO5zV", "name": "data", "output_types": ["Data"]}}, "id": "reactflow__edge-File-DO5zV{\u0153dataType\u0153:\u0153File\u0153,\u0153id\u0153:\u0153File-DO5zV\u0153,\u0153name\u0153:\u0153data\u0153,\u0153output_types\u0153:[\u0153Data\u0153]}-SplitText-RnCyU{\u0153fieldName\u0153:\u0153data_inputs\u0153,\u0153id\u0153:\u0153SplitText-RnCyU\u0153,\u0153inputTypes\u0153:[\u0153Data\u0153],\u0153type\u0153:\u0153other\u0153}", "animated": false, "className": "", "selected": false}, {"source": "HuggingFaceInferenceAPIEmbeddings-Suz5I", "sourceHandle": "{\u0153dataType\u0153:\u0153HuggingFaceInferenceAPIEmbeddings\u0153,\u0153id\u0153:\u0153HuggingFaceInferenceAPIEmbeddings-Suz5I\u0153,\u0153name\u0153:\u0153embeddings\u0153,\u0153output_types\u0153:[\u0153Embeddings\u0153]}", "target": "FAISS-toNtm", "targetHandle": "{\u0153fieldName\u0153:\u0153embedding\u0153,\u0153id\u0153:\u0153FAISS-toNtm\u0153,\u0153inputTypes\u0153:[\u0153Embeddings\u0153],\u0153type\u0153:\u0153other\u0153}", "data": {"targetHandle": {"fieldName": "embedding", "id": "FAISS-toNtm", "inputTypes": ["Embeddings"], "type": "other"}, "sourceHandle": {"dataType": "HuggingFaceInferenceAPIEmbeddings", "id": "HuggingFaceInferenceAPIEmbeddings-Suz5I", "name": "embeddings", "output_types": ["Embeddings"]}}, "id": "reactflow__edge-HuggingFaceInferenceAPIEmbeddings-Suz5I{\u0153dataType\u0153:\u0153HuggingFaceInferenceAPIEmbeddings\u0153,\u0153id\u0153:\u0153HuggingFaceInferenceAPIEmbeddings-Suz5I\u0153,\u0153name\u0153:\u0153embeddings\u0153,\u0153output_types\u0153:[\u0153Embeddings\u0153]}-FAISS-toNtm{\u0153fieldName\u0153:\u0153embedding\u0153,\u0153id\u0153:\u0153FAISS-toNtm\u0153,\u0153inputTypes\u0153:[\u0153Embeddings\u0153],\u0153type\u0153:\u0153other\u0153}", "animated": false, "className": "", "selected": false}, {"source": "SplitText-RnCyU", "sourceHandle": "{\u0153dataType\u0153:\u0153SplitText\u0153,\u0153id\u0153:\u0153SplitText-RnCyU\u0153,\u0153name\u0153:\u0153chunks\u0153,\u0153output_types\u0153:[\u0153Data\u0153]}", "target": "FAISS-toNtm", "targetHandle": "{\u0153fieldName\u0153:\u0153ingest_data\u0153,\u0153id\u0153:\u0153FAISS-toNtm\u0153,\u0153inputTypes\u0153:[\u0153Data\u0153],\u0153type\u0153:\u0153other\u0153}", "data": {"targetHandle": {"fieldName": "ingest_data", "id": "FAISS-toNtm", "inputTypes": ["Data"], "type": "other"}, "sourceHandle": {"dataType": "SplitText", "id": "SplitText-RnCyU", "name": "chunks", "output_types": ["Data"]}}, "id": "reactflow__edge-SplitText-RnCyU{\u0153dataType\u0153:\u0153SplitText\u0153,\u0153id\u0153:\u0153SplitText-RnCyU\u0153,\u0153name\u0153:\u0153chunks\u0153,\u0153output_types\u0153:[\u0153Data\u0153]}-FAISS-toNtm{\u0153fieldName\u0153:\u0153ingest_data\u0153,\u0153id\u0153:\u0153FAISS-toNtm\u0153,\u0153inputTypes\u0153:[\u0153Data\u0153],\u0153type\u0153:\u0153other\u0153}", "animated": false, "className": "", "selected": false}, {"source": "HuggingFaceInferenceAPIEmbeddings-JqfkL", "sourceHandle": "{\u0153dataType\u0153:\u0153HuggingFaceInferenceAPIEmbeddings\u0153,\u0153id\u0153:\u0153HuggingFaceInferenceAPIEmbeddings-JqfkL\u0153,\u0153name\u0153:\u0153embeddings\u0153,\u0153output_types\u0153:[\u0153Embeddings\u0153]}", "target": "FAISS-SqfVh", "targetHandle": "{\u0153fieldName\u0153:\u0153embedding\u0153,\u0153id\u0153:\u0153FAISS-SqfVh\u0153,\u0153inputTypes\u0153:[\u0153Embeddings\u0153],\u0153type\u0153:\u0153other\u0153}", "data": {"targetHandle": {"fieldName": "embedding", "id": "FAISS-SqfVh", "inputTypes": ["Embeddings"], "type": "other"}, "sourceHandle": {"dataType": "HuggingFaceInferenceAPIEmbeddings", "id": "HuggingFaceInferenceAPIEmbeddings-JqfkL", "name": "embeddings", "output_types": ["Embeddings"]}}, "id": "reactflow__edge-HuggingFaceInferenceAPIEmbeddings-JqfkL{\u0153dataType\u0153:\u0153HuggingFaceInferenceAPIEmbeddings\u0153,\u0153id\u0153:\u0153HuggingFaceInferenceAPIEmbeddings-JqfkL\u0153,\u0153name\u0153:\u0153embeddings\u0153,\u0153output_types\u0153:[\u0153Embeddings\u0153]}-FAISS-SqfVh{\u0153fieldName\u0153:\u0153embedding\u0153,\u0153id\u0153:\u0153FAISS-SqfVh\u0153,\u0153inputTypes\u0153:[\u0153Embeddings\u0153],\u0153type\u0153:\u0153other\u0153}", "animated": false, "className": "", "selected": false}, {"source": "FAISS-SqfVh", "sourceHandle": "{\u0153dataType\u0153:\u0153FAISS\u0153,\u0153id\u0153:\u0153FAISS-SqfVh\u0153,\u0153name\u0153:\u0153search_results\u0153,\u0153output_types\u0153:[\u0153Data\u0153]}", "target": "ParseData-cAwrl", "targetHandle": "{\u0153fieldName\u0153:\u0153data\u0153,\u0153id\u0153:\u0153ParseData-cAwrl\u0153,\u0153inputTypes\u0153:[\u0153Data\u0153],\u0153type\u0153:\u0153other\u0153}", "data": {"targetHandle": {"fieldName": "data", "id": "ParseData-cAwrl", "inputTypes": ["Data"], "type": "other"}, "sourceHandle": {"dataType": "FAISS", "id": "FAISS-SqfVh", "name": "search_results", "output_types": ["Data"]}}, "id": "reactflow__edge-FAISS-SqfVh{\u0153dataType\u0153:\u0153FAISS\u0153,\u0153id\u0153:\u0153FAISS-SqfVh\u0153,\u0153name\u0153:\u0153search_results\u0153,\u0153output_types\u0153:[\u0153Data\u0153]}-ParseData-cAwrl{\u0153fieldName\u0153:\u0153data\u0153,\u0153id\u0153:\u0153ParseData-cAwrl\u0153,\u0153inputTypes\u0153:[\u0153Data\u0153],\u0153type\u0153:\u0153other\u0153}", "animated": false, "className": "", "selected": false}, {"source": "ParseData-cAwrl", "sourceHandle": "{\u0153dataType\u0153:\u0153ParseData\u0153,\u0153id\u0153:\u0153ParseData-cAwrl\u0153,\u0153name\u0153:\u0153text\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}", "target": "Prompt-TGd3y", "targetHandle": "{\u0153fieldName\u0153:\u0153context\u0153,\u0153id\u0153:\u0153Prompt-TGd3y\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "data": {"targetHandle": {"fieldName": "context", "id": "Prompt-TGd3y", "inputTypes": ["Message"], "type": "str"}, "sourceHandle": {"dataType": "ParseData", "id": "ParseData-cAwrl", "name": "text", "output_types": ["Message"]}}, "id": "reactflow__edge-ParseData-cAwrl{\u0153dataType\u0153:\u0153ParseData\u0153,\u0153id\u0153:\u0153ParseData-cAwrl\u0153,\u0153name\u0153:\u0153text\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}-Prompt-TGd3y{\u0153fieldName\u0153:\u0153context\u0153,\u0153id\u0153:\u0153Prompt-TGd3y\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "animated": false, "className": "", "selected": false}, {"source": "Prompt-TGd3y", "sourceHandle": "{\u0153dataType\u0153:\u0153Prompt\u0153,\u0153id\u0153:\u0153Prompt-TGd3y\u0153,\u0153name\u0153:\u0153prompt\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}", "target": "OpenAIModel-botx1", "targetHandle": "{\u0153fieldName\u0153:\u0153input_value\u0153,\u0153id\u0153:\u0153OpenAIModel-botx1\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "data": {"targetHandle": {"fieldName": "input_value", "id": "OpenAIModel-botx1", "inputTypes": ["Message"], "type": "str"}, "sourceHandle": {"dataType": "Prompt", "id": "Prompt-TGd3y", "name": "prompt", "output_types": ["Message"]}}, "id": "reactflow__edge-Prompt-TGd3y{\u0153dataType\u0153:\u0153Prompt\u0153,\u0153id\u0153:\u0153Prompt-TGd3y\u0153,\u0153name\u0153:\u0153prompt\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}-OpenAIModel-botx1{\u0153fieldName\u0153:\u0153input_value\u0153,\u0153id\u0153:\u0153OpenAIModel-botx1\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "animated": false, "className": "", "selected": false}, {"source": "SplitText-RnCyU", "sourceHandle": "{\u0153dataType\u0153:\u0153SplitText\u0153,\u0153id\u0153:\u0153SplitText-RnCyU\u0153,\u0153name\u0153:\u0153chunks\u0153,\u0153output_types\u0153:[\u0153Data\u0153]}", "target": "FAISS-SqfVh", "targetHandle": "{\u0153fieldName\u0153:\u0153ingest_data\u0153,\u0153id\u0153:\u0153FAISS-SqfVh\u0153,\u0153inputTypes\u0153:[\u0153Data\u0153],\u0153type\u0153:\u0153other\u0153}", "data": {"targetHandle": {"fieldName": "ingest_data", "id": "FAISS-SqfVh", "inputTypes": ["Data"], "type": "other"}, "sourceHandle": {"dataType": "SplitText", "id": "SplitText-RnCyU", "name": "chunks", "output_types": ["Data"]}}, "id": "reactflow__edge-SplitText-RnCyU{\u0153dataType\u0153:\u0153SplitText\u0153,\u0153id\u0153:\u0153SplitText-RnCyU\u0153,\u0153name\u0153:\u0153chunks\u0153,\u0153output_types\u0153:[\u0153Data\u0153]}-FAISS-SqfVh{\u0153fieldName\u0153:\u0153ingest_data\u0153,\u0153id\u0153:\u0153FAISS-SqfVh\u0153,\u0153inputTypes\u0153:[\u0153Data\u0153],\u0153type\u0153:\u0153other\u0153}", "animated": false, "className": ""}, {"source": "JSONCleaner-1mD4u", "sourceHandle": "{\u0153dataType\u0153:\u0153JSONCleaner\u0153,\u0153id\u0153:\u0153JSONCleaner-1mD4u\u0153,\u0153name\u0153:\u0153output\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}", "target": "ConditionalRouter-Wxm5a", "targetHandle": "{\u0153fieldName\u0153:\u0153input_text\u0153,\u0153id\u0153:\u0153ConditionalRouter-Wxm5a\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "data": {"targetHandle": {"fieldName": "input_text", "id": "ConditionalRouter-Wxm5a", "inputTypes": ["Message"], "type": "str"}, "sourceHandle": {"dataType": "JSONCleaner", "id": "JSONCleaner-1mD4u", "name": "output", "output_types": ["Message"]}}, "id": "reactflow__edge-JSONCleaner-1mD4u{\u0153dataType\u0153:\u0153JSONCleaner\u0153,\u0153id\u0153:\u0153JSONCleaner-1mD4u\u0153,\u0153name\u0153:\u0153output\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}-ConditionalRouter-Wxm5a{\u0153fieldName\u0153:\u0153input_text\u0153,\u0153id\u0153:\u0153ConditionalRouter-Wxm5a\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "animated": false, "className": ""}, {"source": "ConditionalRouter-Wxm5a", "sourceHandle": "{\u0153dataType\u0153:\u0153ConditionalRouter\u0153,\u0153id\u0153:\u0153ConditionalRouter-Wxm5a\u0153,\u0153name\u0153:\u0153false_result\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}", "target": "TextOutput-5j9L7", "targetHandle": "{\u0153fieldName\u0153:\u0153input_value\u0153,\u0153id\u0153:\u0153TextOutput-5j9L7\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "data": {"targetHandle": {"fieldName": "input_value", "id": "TextOutput-5j9L7", "inputTypes": ["Message"], "type": "str"}, "sourceHandle": {"dataType": "ConditionalRouter", "id": "ConditionalRouter-Wxm5a", "name": "false_result", "output_types": ["Message"]}}, "id": "reactflow__edge-ConditionalRouter-Wxm5a{\u0153dataType\u0153:\u0153ConditionalRouter\u0153,\u0153id\u0153:\u0153ConditionalRouter-Wxm5a\u0153,\u0153name\u0153:\u0153false_result\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}-TextOutput-5j9L7{\u0153fieldName\u0153:\u0153input_value\u0153,\u0153id\u0153:\u0153TextOutput-5j9L7\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "animated": false, "className": ""}, {"source": "TextOutput-5j9L7", "sourceHandle": "{\u0153dataType\u0153:\u0153TextOutput\u0153,\u0153id\u0153:\u0153TextOutput-5j9L7\u0153,\u0153name\u0153:\u0153text\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}", "target": "JSONCleaner-rzxHY", "targetHandle": "{\u0153fieldName\u0153:\u0153json_str\u0153,\u0153id\u0153:\u0153JSONCleaner-rzxHY\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "data": {"targetHandle": {"fieldName": "json_str", "id": "JSONCleaner-rzxHY", "inputTypes": ["Message"], "type": "str"}, "sourceHandle": {"dataType": "TextOutput", "id": "TextOutput-5j9L7", "name": "text", "output_types": ["Message"]}}, "id": "reactflow__edge-TextOutput-5j9L7{\u0153dataType\u0153:\u0153TextOutput\u0153,\u0153id\u0153:\u0153TextOutput-5j9L7\u0153,\u0153name\u0153:\u0153text\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}-JSONCleaner-rzxHY{\u0153fieldName\u0153:\u0153json_str\u0153,\u0153id\u0153:\u0153JSONCleaner-rzxHY\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "animated": false, "className": ""}, {"source": "JSONCleaner-rzxHY", "sourceHandle": "{\u0153dataType\u0153:\u0153JSONCleaner\u0153,\u0153id\u0153:\u0153JSONCleaner-rzxHY\u0153,\u0153name\u0153:\u0153output\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}", "target": "ChatOutput-WG4uv", "targetHandle": "{\u0153fieldName\u0153:\u0153input_value\u0153,\u0153id\u0153:\u0153ChatOutput-WG4uv\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "data": {"targetHandle": {"fieldName": "input_value", "id": "ChatOutput-WG4uv", "inputTypes": ["Message"], "type": "str"}, "sourceHandle": {"dataType": "JSONCleaner", "id": "JSONCleaner-rzxHY", "name": "output", "output_types": ["Message"]}}, "id": "reactflow__edge-JSONCleaner-rzxHY{\u0153dataType\u0153:\u0153JSONCleaner\u0153,\u0153id\u0153:\u0153JSONCleaner-rzxHY\u0153,\u0153name\u0153:\u0153output\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}-ChatOutput-WG4uv{\u0153fieldName\u0153:\u0153input_value\u0153,\u0153id\u0153:\u0153ChatOutput-WG4uv\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "animated": false, "className": ""}, {"source": "ConditionalRouter-Wxm5a", "sourceHandle": "{\u0153dataType\u0153:\u0153ConditionalRouter\u0153,\u0153id\u0153:\u0153ConditionalRouter-Wxm5a\u0153,\u0153name\u0153:\u0153true_result\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}", "target": "TextOutput-1jQWG", "targetHandle": "{\u0153fieldName\u0153:\u0153input_value\u0153,\u0153id\u0153:\u0153TextOutput-1jQWG\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "data": {"targetHandle": {"fieldName": "input_value", "id": "TextOutput-1jQWG", "inputTypes": ["Message"], "type": "str"}, "sourceHandle": {"dataType": "ConditionalRouter", "id": "ConditionalRouter-Wxm5a", "name": "true_result", "output_types": ["Message"]}}, "id": "reactflow__edge-ConditionalRouter-Wxm5a{\u0153dataType\u0153:\u0153ConditionalRouter\u0153,\u0153id\u0153:\u0153ConditionalRouter-Wxm5a\u0153,\u0153name\u0153:\u0153true_result\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}-TextOutput-1jQWG{\u0153fieldName\u0153:\u0153input_value\u0153,\u0153id\u0153:\u0153TextOutput-1jQWG\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "animated": false, "className": ""}, {"source": "TextOutput-1jQWG", "sourceHandle": "{\u0153dataType\u0153:\u0153TextOutput\u0153,\u0153id\u0153:\u0153TextOutput-1jQWG\u0153,\u0153name\u0153:\u0153text\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}", "target": "FAISS-SqfVh", "targetHandle": "{\u0153fieldName\u0153:\u0153search_query\u0153,\u0153id\u0153:\u0153FAISS-SqfVh\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "data": {"targetHandle": {"fieldName": "search_query", "id": "FAISS-SqfVh", "inputTypes": ["Message"], "type": "str"}, "sourceHandle": {"dataType": "TextOutput", "id": "TextOutput-1jQWG", "name": "text", "output_types": ["Message"]}}, "id": "reactflow__edge-TextOutput-1jQWG{\u0153dataType\u0153:\u0153TextOutput\u0153,\u0153id\u0153:\u0153TextOutput-1jQWG\u0153,\u0153name\u0153:\u0153text\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}-FAISS-SqfVh{\u0153fieldName\u0153:\u0153search_query\u0153,\u0153id\u0153:\u0153FAISS-SqfVh\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "animated": false, "className": ""}, {"source": "TextOutput-1jQWG", "sourceHandle": "{\u0153dataType\u0153:\u0153TextOutput\u0153,\u0153id\u0153:\u0153TextOutput-1jQWG\u0153,\u0153name\u0153:\u0153text\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}", "target": "Prompt-TGd3y", "targetHandle": "{\u0153fieldName\u0153:\u0153query\u0153,\u0153id\u0153:\u0153Prompt-TGd3y\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "data": {"targetHandle": {"fieldName": "query", "id": "Prompt-TGd3y", "inputTypes": ["Message"], "type": "str"}, "sourceHandle": {"dataType": "TextOutput", "id": "TextOutput-1jQWG", "name": "text", "output_types": ["Message"]}}, "id": "reactflow__edge-TextOutput-1jQWG{\u0153dataType\u0153:\u0153TextOutput\u0153,\u0153id\u0153:\u0153TextOutput-1jQWG\u0153,\u0153name\u0153:\u0153text\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}-Prompt-TGd3y{\u0153fieldName\u0153:\u0153query\u0153,\u0153id\u0153:\u0153Prompt-TGd3y\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "animated": false, "className": ""}, {"source": "OpenAIModel-botx1", "sourceHandle": "{\u0153dataType\u0153:\u0153OpenAIModel\u0153,\u0153id\u0153:\u0153OpenAIModel-botx1\u0153,\u0153name\u0153:\u0153text_output\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}", "target": "ReplaceText-nHslZ", "targetHandle": "{\u0153fieldName\u0153:\u0153text2\u0153,\u0153id\u0153:\u0153ReplaceText-nHslZ\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "data": {"targetHandle": {"fieldName": "text2", "id": "ReplaceText-nHslZ", "inputTypes": ["Message"], "type": "str"}, "sourceHandle": {"dataType": "OpenAIModel", "id": "OpenAIModel-botx1", "name": "text_output", "output_types": ["Message"]}}, "id": "reactflow__edge-OpenAIModel-botx1{\u0153dataType\u0153:\u0153OpenAIModel\u0153,\u0153id\u0153:\u0153OpenAIModel-botx1\u0153,\u0153name\u0153:\u0153text_output\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}-ReplaceText-nHslZ{\u0153fieldName\u0153:\u0153text2\u0153,\u0153id\u0153:\u0153ReplaceText-nHslZ\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "animated": false, "className": ""}, {"source": "JSONCleaner-1mD4u", "sourceHandle": "{\u0153dataType\u0153:\u0153JSONCleaner\u0153,\u0153id\u0153:\u0153JSONCleaner-1mD4u\u0153,\u0153name\u0153:\u0153output\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}", "target": "ReplaceText-nHslZ", "targetHandle": "{\u0153fieldName\u0153:\u0153text1\u0153,\u0153id\u0153:\u0153ReplaceText-nHslZ\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "data": {"targetHandle": {"fieldName": "text1", "id": "ReplaceText-nHslZ", "inputTypes": ["Message"], "type": "str"}, "sourceHandle": {"dataType": "JSONCleaner", "id": "JSONCleaner-1mD4u", "name": "output", "output_types": ["Message"]}}, "id": "reactflow__edge-JSONCleaner-1mD4u{\u0153dataType\u0153:\u0153JSONCleaner\u0153,\u0153id\u0153:\u0153JSONCleaner-1mD4u\u0153,\u0153name\u0153:\u0153output\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}-ReplaceText-nHslZ{\u0153fieldName\u0153:\u0153text1\u0153,\u0153id\u0153:\u0153ReplaceText-nHslZ\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "animated": false, "className": ""}, {"source": "ReplaceText-nHslZ", "sourceHandle": "{\u0153dataType\u0153:\u0153ReplaceText\u0153,\u0153id\u0153:\u0153ReplaceText-nHslZ\u0153,\u0153name\u0153:\u0153modified_text\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}", "target": "JSONCleaner-Z9K0c", "targetHandle": "{\u0153fieldName\u0153:\u0153json_str\u0153,\u0153id\u0153:\u0153JSONCleaner-Z9K0c\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "data": {"targetHandle": {"fieldName": "json_str", "id": "JSONCleaner-Z9K0c", "inputTypes": ["Message"], "type": "str"}, "sourceHandle": {"dataType": "ReplaceText", "id": "ReplaceText-nHslZ", "name": "modified_text", "output_types": ["Message"]}}, "id": "reactflow__edge-ReplaceText-nHslZ{\u0153dataType\u0153:\u0153ReplaceText\u0153,\u0153id\u0153:\u0153ReplaceText-nHslZ\u0153,\u0153name\u0153:\u0153modified_text\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}-JSONCleaner-Z9K0c{\u0153fieldName\u0153:\u0153json_str\u0153,\u0153id\u0153:\u0153JSONCleaner-Z9K0c\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "animated": false, "className": ""}, {"source": "JSONCleaner-Z9K0c", "sourceHandle": "{\u0153dataType\u0153:\u0153JSONCleaner\u0153,\u0153id\u0153:\u0153JSONCleaner-Z9K0c\u0153,\u0153name\u0153:\u0153output\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}", "target": "ChatOutput-v8dd1", "targetHandle": "{\u0153fieldName\u0153:\u0153input_value\u0153,\u0153id\u0153:\u0153ChatOutput-v8dd1\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "data": {"targetHandle": {"fieldName": "input_value", "id": "ChatOutput-v8dd1", "inputTypes": ["Message"], "type": "str"}, "sourceHandle": {"dataType": "JSONCleaner", "id": "JSONCleaner-Z9K0c", "name": "output", "output_types": ["Message"]}}, "id": "reactflow__edge-JSONCleaner-Z9K0c{\u0153dataType\u0153:\u0153JSONCleaner\u0153,\u0153id\u0153:\u0153JSONCleaner-Z9K0c\u0153,\u0153name\u0153:\u0153output\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}-ChatOutput-v8dd1{\u0153fieldName\u0153:\u0153input_value\u0153,\u0153id\u0153:\u0153ChatOutput-v8dd1\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "animated": false, "className": ""}], "viewport": {"x": 120.20843260143607, "y": 165.4584231896263, "zoom": 0.17406136783819762}}, "is_component": false, "updated_at": "2025-03-24T05:42:11+00:00", "webhook": false, "endpoint_name": null, "tags": null, "locked": false, "id": "0e567632-824e-4488-8878-2689a3a2334f", "user_id": "24e4b9b7-973b-4d32-8618-2f1b6b6b61f9", "folder_id": "a91f2823-6f82-4a9b-b6ed-222b8ef6e7f1"}